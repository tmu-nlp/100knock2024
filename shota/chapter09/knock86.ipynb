{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3LQlfugXBRi",
        "outputId": "d2b7bd4c-65ce-4b91-f41b-f5d497d16ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "学習データ\n",
            "CATEGORY\n",
            "b    4502\n",
            "e    4223\n",
            "t    1219\n",
            "m     728\n",
            "Name: count, dtype: int64\n",
            "検証データ\n",
            "CATEGORY\n",
            "b    562\n",
            "e    528\n",
            "t    153\n",
            "m     91\n",
            "Name: count, dtype: int64\n",
            "評価データ\n",
            "CATEGORY\n",
            "b    563\n",
            "e    528\n",
            "t    152\n",
            "m     91\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#データの読み込み\n",
        "import pandas as pd\n",
        "train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/chapter09/train.txt', sep=\"\\t\")\n",
        "test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/chapter09/test.txt', sep=\"\\t\")\n",
        "valid = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/chapter09/valid.txt', sep=\"\\t\")\n",
        "# データ数の確認\n",
        "print('学習データ')\n",
        "print(train['CATEGORY'].value_counts())\n",
        "print('検証データ')\n",
        "print(valid['CATEGORY'].value_counts())\n",
        "print('評価データ')\n",
        "print(test['CATEGORY'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 単語の辞書を作成\n",
        "from collections import Counter\n",
        "words = []\n",
        "for text in train['TITLE']:                #訓練データから文章を1つずつ取り出す\n",
        "    for word in text.rstrip().split():     #文章を単語に分解\n",
        "        words.append(word)                 #単語をリストに追加\n",
        "c = Counter(words)                         #単語の出現回数を数える\n",
        "print(c.most_common(10))                   #頻度上位10単語\n",
        "word2id = {}                               #単語IDの辞書\n",
        "for i, cnt in enumerate(c.most_common()):  #頻度上位10単語分繰り返す\n",
        "    if cnt[1] > 1:                         #出現回数が1より大きい単語のみ\n",
        "        word2id[cnt[0]] = i + 1            #辞書に単語とIDを紐付ける\n",
        "for i, cnt in enumerate(word2id.items()):  #辞書の中身を確認\n",
        "    if i >= 10:                            #10単語だけ表示\n",
        "        break                              #for文を抜ける\n",
        "    print(cnt[0], cnt[1])                  #単語とIDを表示"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaoFH2FcaZcJ",
        "outputId": "12b687be-da12-42ea-ff91-e1711c2801be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('to', 2151), ('...', 2031), ('in', 1415), ('as', 1027), ('on', 1025), ('UPDATE', 1000), ('-', 991), ('for', 969), ('of', 957), ('The', 859)]\n",
            "to 1\n",
            "... 2\n",
            "in 3\n",
            "as 4\n",
            "on 5\n",
            "UPDATE 6\n",
            "- 7\n",
            "for 8\n",
            "of 9\n",
            "The 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 単語のID化\n",
        "def tokenizer(text):                                 #単語IDのリストを返す関数\n",
        "    words = text.rstrip().split()                    #単語に分解\n",
        "    return [word2id.get(word, 0) for word in words]  #単語のIDに変換\n",
        "\n",
        "sample = train.at[0, 'TITLE']                        #学習データの1つ目の文章\n",
        "print(sample)                                        #文章を表示\n",
        "print(tokenizer(sample))                             #文章を単語IDに変換"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-hhnRWjagOZ",
        "outputId": "22eb39e1-08ad-48d3-987e-65c038720d51"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justin Bieber Under Investigation For Attempted Robbery At Dave & Buster's\n",
            "[66, 79, 733, 2094, 21, 4933, 6674, 35, 1514, 86, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNNの作成\n",
        "# モデルの構築\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as data\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "\n",
        "# 乱数のシードを設定\n",
        "# parserなどで指定\n",
        "seed = 1234\n",
        "\n",
        "random.seed(seed)                                   # Python標準ライブラリの乱数のシードを設定\n",
        "np.random.seed(seed)                                # Numpy乱数のシードを設定\n",
        "torch.manual_seed(seed)                             # PyTorch乱数のシードを設定\n",
        "torch.cuda.manual_seed(seed)                        # PyTorchのCUDA乱数のシードを設定\n",
        "torch.backends.cudnn.benchmark = False              # PyTorchのCUDNNのベンチマークを使用しない  (cudnn内の非決定的な処理の固定化)\n",
        "torch.backends.cudnn.deterministic = True           # PyTorchのCUDNNの定着を使用\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32      # 乱数生成のシードの初期値を設定\n",
        "    np.random.seed(worker_seed)                     # Numpy乱数のシードを設定\n",
        "    random.seed(worker_seed)                        # Python標準ライブラリの乱数のシードを設定\n",
        "\n",
        "g = torch.Generator()                               # PyTorch乱数のシードを設定\n",
        "g.manual_seed(seed)                                 # 乱数生成器にシードを設定"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY0GKwcpKlfN",
        "outputId": "6800f591-1306-4fe9-bfde-568e72762ce1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7df746d78d30>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([tokenizer(sample)], dtype=torch.int64)                           # 文章を単語IDに変換\n",
        "print(x)                                                                           # 文章をIDでを表示\n",
        "print(x.size())                                                                    # 文章のサイズを確認"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-8OkqkTLAXg",
        "outputId": "49e03136-fe88-47cd-a34e-37ce148eb03a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  66,   79,  733, 2094,   21, 4933, 6674,   35, 1514,   86,    0]])\n",
            "torch.Size([1, 11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ターゲットのテンソル化\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "Y_train = torch.from_numpy(train['CATEGORY'].map(category_dict).values)\n",
        "Y_valid = torch.from_numpy(valid['CATEGORY'].map(category_dict).values)\n",
        "Y_test = torch.from_numpy(test['CATEGORY'].map(category_dict).values)\n",
        "print(Y_train.size())\n",
        "print(Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhA2l-VwAsr_",
        "outputId": "7f4eaf51-d2aa-4157-d525-2c179d99617f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10672])\n",
            "tensor([2, 0, 2,  ..., 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    newsのDatasetクラス\n",
        "\n",
        "    Attributes\n",
        "    ----------------------------\n",
        "    X : データフレーム\n",
        "        単語ベクトルの平均をまとめたテンソル\n",
        "    y : テンソル\n",
        "        カテゴリをラベル化したテンソル\n",
        "    phase : 'train' or 'val'\n",
        "        学習か訓練かを設定する\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, phase='train'):\n",
        "        self.X = X['TITLE']\n",
        "        self.y = y\n",
        "        self.phase = phase\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"全データサイズを返す\"\"\"\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"idxに対応するテンソル形式のデータとラベルを取得\"\"\"\n",
        "        inputs = torch.tensor(tokenizer(self.X[idx]))\n",
        "        return inputs, self.y[idx]\n",
        "\n",
        "train_dataset = NewsDataset(train, Y_train, phase='train')\n",
        "valid_dataset = NewsDataset(valid, Y_valid, phase='val')\n",
        "test_dataset = NewsDataset(test, Y_test, phase='val')\n",
        "# 動作確認\n",
        "idx = 0\n",
        "print(train_dataset.__getitem__(idx)[0].size())\n",
        "print(train_dataset.__getitem__(idx)[1])\n",
        "print(valid_dataset.__getitem__(idx)[0].size())\n",
        "print(valid_dataset.__getitem__(idx)[1])\n",
        "print(test_dataset.__getitem__(idx)[0].size())\n",
        "print(test_dataset.__getitem__(idx)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMifdMwaAvji",
        "outputId": "81a9ba7b-f0c5-4270-addc-b209b3aaf523"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11])\n",
            "tensor(2)\n",
            "torch.Size([11])\n",
            "tensor(3)\n",
            "torch.Size([13])\n",
            "tensor(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    sequences = [x[0] for x in batch]\n",
        "    labels = torch.LongTensor([x[1] for x in batch])\n",
        "    x = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=PADDING_IDX)\n",
        "    return x, labels\n",
        "\n",
        "# DataLoaderを作成\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, worker_init_fn=seed_worker, generator=g)\n",
        "valid_dataloader = data.DataLoader(\n",
        "            valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, worker_init_fn=seed_worker, generator=g)\n",
        "test_dataloader = data.DataLoader(\n",
        "            test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "dataloaders_dict = {'train': train_dataloader,\n",
        "                    'val': valid_dataloader,\n",
        "                    'test': test_dataloader,\n",
        "                   }\n",
        "\n",
        "# 動作確認\n",
        "batch_iter = iter(dataloaders_dict['train'])\n",
        "inputs, labels = next(batch_iter)\n",
        "print(inputs)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRSh5D22Axsg",
        "outputId": "2ef056d9-2c38-4f53-c486-8e50ec595c8d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1136,   890,    22,  ..., 10327, 10327, 10327],\n",
            "        [ 9241,   853,  8128,  ..., 10327, 10327, 10327],\n",
            "        [  211,  1843,   104,  ..., 10327, 10327, 10327],\n",
            "        ...,\n",
            "        [ 2886,  4097,  5178,  ..., 10327, 10327, 10327],\n",
            "        [ 2595,    40,  8576,  ..., 10327, 10327, 10327],\n",
            "        [    6,     0,  3373,  ..., 10327, 10327, 10327]])\n",
            "tensor([2, 2, 0, 0, 2, 1, 0, 0, 2, 1, 1, 1, 0, 2, 0, 0, 0, 0, 1, 2, 2, 2, 0, 1,\n",
            "        0, 0, 1, 2, 1, 2, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 3, 2, 3, 1, 2, 0, 2,\n",
            "        0, 0, 1, 2, 2, 0, 0, 2, 2, 2, 2, 1, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# 学習済みモデルのロード\n",
        "file = '/content/drive/MyDrive/Colab Notebooks/chapter09/GoogleNews-vectors-negative300.bin.gz'\n",
        "model = KeyedVectors.load_word2vec_format(file, binary=True)\n",
        "\n",
        "# 学習済み単語ベクトルの取得\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 2\n",
        "EMB_SIZE = 300\n",
        "weights = np.zeros((VOCAB_SIZE, EMB_SIZE))\n",
        "words_in_pretrained = 0\n",
        "for i, word in enumerate(word2id.keys()):\n",
        "    try:\n",
        "        weights[i] = model[word]\n",
        "        words_in_pretrained += 1\n",
        "    except KeyError:\n",
        "        weights[i] = np.random.normal(scale=0.1, size=(EMB_SIZE,))\n",
        "weights = torch.from_numpy(weights.astype((np.float32)))\n",
        "\n",
        "print(f'学習済みベクトル利用単語数: {words_in_pretrained} / {VOCAB_SIZE}')\n",
        "print(weights.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLn_cZ71QGqD",
        "outputId": "29eba55b-a1c1-4879-88c7-2e6005d674ed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "学習済みベクトル利用単語数: 8340 / 10328\n",
            "torch.Size([10328, 300])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_acc(net, dataloader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    net.eval()\n",
        "    corrects = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            _, preds = torch.max(outputs, 1) # ラベルを予想\n",
        "            corrects += torch.sum(preds == labels.data).cpu()\n",
        "    return corrects / len(dataloader.dataset)"
      ],
      "metadata": {
        "id": "orK8m3eSRY11"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習を実行する\n",
        "# 学習用の関数を定義\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # 初期設定\n",
        "    # GPUが使えるか確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(torch.cuda.get_device_name())\n",
        "    print(\"使用デバイス:\", device)\n",
        "\n",
        "    # ネットワークをgpuへ\n",
        "    net.to(device)\n",
        "\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    valid_loss = []\n",
        "    valid_acc = []\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの学習と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train() # 訓練モード\n",
        "            else:\n",
        "                net.eval() # 検証モード\n",
        "\n",
        "            epoch_loss = 0.0 # epochの損失和\n",
        "            epoch_corrects = 0 # epochの正解数\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for inputs, labels in dataloaders_dict[phase]:\n",
        "                # GPUが使えるならGPUにおっくる\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad() # optimizerを初期化\n",
        "\n",
        "                # 順伝播計算(forward)\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(inputs)\n",
        "                    loss = criterion(outputs, labels) # 損失を計算\n",
        "                    _, preds = torch.max(outputs, 1) # ラベルを予想\n",
        "\n",
        "                    # 訓練時は逆伝播\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # イテレーション結果の計算\n",
        "                    # lossの合計を更新\n",
        "                    epoch_loss += loss.item() * inputs.size(0)\n",
        "                    # 正解数の合計を更新\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率の表示\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "                train_acc.append(epoch_acc.cpu())\n",
        "            else:\n",
        "                valid_loss.append(epoch_loss)\n",
        "                valid_acc.append(epoch_acc.cpu())\n",
        "\n",
        "        print('Epoch {} / {} (train) Loss: {:.4f}, Acc: {:.4f}, (val) Loss: {:.4f}, Acc: {:.4f}'.format(epoch + 1, num_epochs, train_loss[-1], train_acc[-1], valid_loss[-1], valid_acc[-1]))\n",
        "    return train_loss, train_acc, valid_loss, valid_acc"
      ],
      "metadata": {
        "id": "T-hgB0t7Tf_h"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, kernel_heights, stride, padding, emb_weights=None):\n",
        "        super().__init__()\n",
        "        if emb_weights != None:  # 指定があれば埋め込み層の重みをemb_weightsで初期化\n",
        "            self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n",
        "        else:\n",
        "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "        self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding, 0))\n",
        "        self.drop = nn.Dropout(0.4)\n",
        "        self.fc = nn.Linear(out_channels, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.emb(x).unsqueeze(1)\n",
        "        conv = self.conv(emb)\n",
        "        act = F.relu(conv.squeeze(3))\n",
        "        max_pool = F.max_pool1d(act, act.size()[2])\n",
        "        logits = self.fc(self.drop(max_pool.squeeze(2)))\n",
        "        return logits\n",
        "\n",
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 2\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(word2id.values())) + 1\n",
        "OUTPUT_SIZE = 4\n",
        "OUT_CHANNELS = 100\n",
        "KERNEL_HEIGHTS = 3\n",
        "STRIDE = 1\n",
        "PADDING = 1\n",
        "\n",
        "# モデルの定義\n",
        "model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=weights)\n",
        "x = torch.tensor([tokenizer(sample)], dtype=torch.int64)\n",
        "print(x)\n",
        "print(x.size())\n",
        "print(nn.Softmax(dim=-1)(model(x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWmrRsDCXMxQ",
        "outputId": "272f7326-e93b-49e9-e690-12824e7737f1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  66,   79,  733, 2094,   21, 4933, 6674,   35, 1514,   86,    0]])\n",
            "torch.Size([1, 11])\n",
            "tensor([[0.2729, 0.2647, 0.2208, 0.2416]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "D3W_cKr_XQdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchinfo"
      ],
      "metadata": {
        "id": "GJp3iMQQKqVL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
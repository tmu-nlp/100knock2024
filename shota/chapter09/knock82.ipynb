{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3LQlfugXBRi",
        "outputId": "84595548-47bb-4b9b-c835-4285108a49d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "学習データ\n",
            "CATEGORY\n",
            "b    4502\n",
            "e    4223\n",
            "t    1219\n",
            "m     728\n",
            "Name: count, dtype: int64\n",
            "検証データ\n",
            "CATEGORY\n",
            "b    562\n",
            "e    528\n",
            "t    153\n",
            "m     91\n",
            "Name: count, dtype: int64\n",
            "評価データ\n",
            "CATEGORY\n",
            "b    563\n",
            "e    528\n",
            "t    152\n",
            "m     91\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#データの読み込み\n",
        "import pandas as pd\n",
        "train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/chapter09/train.txt', sep=\"\\t\")\n",
        "test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/chapter09/test.txt', sep=\"\\t\")\n",
        "valid = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/chapter09/valid.txt', sep=\"\\t\")\n",
        "# データ数の確認\n",
        "print('学習データ')\n",
        "print(train['CATEGORY'].value_counts())\n",
        "print('検証データ')\n",
        "print(valid['CATEGORY'].value_counts())\n",
        "print('評価データ')\n",
        "print(test['CATEGORY'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 単語の辞書を作成\n",
        "from collections import Counter\n",
        "words = []\n",
        "for text in train['TITLE']:                #訓練データから文章を1つずつ取り出す\n",
        "    for word in text.rstrip().split():     #文章を単語に分解\n",
        "        words.append(word)                 #単語をリストに追加\n",
        "c = Counter(words)                         #単語の出現回数を数える\n",
        "print(c.most_common(10))                   #頻度上位10単語\n",
        "word2id = {}                               #単語IDの辞書\n",
        "for i, cnt in enumerate(c.most_common()):  #頻度上位10単語分繰り返す\n",
        "    if cnt[1] > 1:                         #出現回数が1より大きい単語のみ\n",
        "        word2id[cnt[0]] = i + 1            #辞書に単語とIDを紐付ける\n",
        "for i, cnt in enumerate(word2id.items()):  #辞書の中身を確認\n",
        "    if i >= 10:                            #10単語だけ表示\n",
        "        break                              #for文を抜ける\n",
        "    print(cnt[0], cnt[1])                  #単語とIDを表示"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaoFH2FcaZcJ",
        "outputId": "0fde9133-8b9c-4063-de4f-5691b0d2f595"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('to', 2151), ('...', 2031), ('in', 1415), ('as', 1027), ('on', 1025), ('UPDATE', 1000), ('-', 991), ('for', 969), ('of', 957), ('The', 859)]\n",
            "to 1\n",
            "... 2\n",
            "in 3\n",
            "as 4\n",
            "on 5\n",
            "UPDATE 6\n",
            "- 7\n",
            "for 8\n",
            "of 9\n",
            "The 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 単語のID化\n",
        "def tokenizer(text):                                 #単語IDのリストを返す関数\n",
        "    words = text.rstrip().split()                    #単語に分解\n",
        "    return [word2id.get(word, 0) for word in words]  #単語のIDに変換\n",
        "\n",
        "sample = train.at[0, 'TITLE']                        #学習データの1つ目の文章\n",
        "print(sample)                                        #文章を表示\n",
        "print(tokenizer(sample))                             #文章を単語IDに変換"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-hhnRWjagOZ",
        "outputId": "98aa0663-d38b-4ef5-a39a-13aeb0c16783"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justin Bieber Under Investigation For Attempted Robbery At Dave & Buster's\n",
            "[66, 79, 733, 2094, 21, 4933, 6674, 35, 1514, 86, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNNの作成\n",
        "# モデルの構築\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as data\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "\n",
        "# 乱数のシードを設定\n",
        "# parserなどで指定\n",
        "seed = 1234\n",
        "\n",
        "random.seed(seed)                                   # Python標準ライブラリの乱数のシードを設定\n",
        "np.random.seed(seed)                                # Numpy乱数のシードを設定\n",
        "torch.manual_seed(seed)                             # PyTorch乱数のシードを設定\n",
        "torch.cuda.manual_seed(seed)                        # PyTorchのCUDA乱数のシードを設定\n",
        "torch.backends.cudnn.benchmark = False              # PyTorchのCUDNNのベンチマークを使用しない  (cudnn内の非決定的な処理の固定化)\n",
        "torch.backends.cudnn.deterministic = True           # PyTorchのCUDNNの定着を使用\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32      # 乱数生成のシードの初期値を設定\n",
        "    np.random.seed(worker_seed)                     # Numpy乱数のシードを設定\n",
        "    random.seed(worker_seed)                        # Python標準ライブラリの乱数のシードを設定\n",
        "\n",
        "g = torch.Generator()                               # PyTorch乱数のシードを設定\n",
        "g.manual_seed(seed)                                 # 乱数生成器にシードを設定\n",
        "\n",
        "class RNN(nn.Module):                                                                               # RNNクラスを定義\n",
        "    def __init__(self, vocab_size, emb_size, padding_idx, hidden_size, output_size, num_layers=1):  # コンストラクタ\n",
        "        super().__init__()                                                                          # 親クラスのコンストラクタを呼ぶ\n",
        "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)                      # 単語埋め込み層\n",
        "        self.rnn = nn.LSTM(emb_size, hidden_size, batch_first=True)                                 # RNN層\n",
        "        self.fc = nn.Linear(hidden_size, output_size)                                               # 全結合層\n",
        "\n",
        "    def forward(self, x, h0=None):                                                                  # 順伝播処理\n",
        "        x = self.emb(x)                                                                             # 単語埋め込み\n",
        "        x, h = self.rnn(x, h0)                                                                      # RNN\n",
        "        x = x[:, -1, :]                                                                             # 最後のステップのみを抽出\n",
        "        logits = self.fc(x)                                                                         # 全結合層\n",
        "        return logits                                                                               # 出力\n",
        "\n",
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 2  # 辞書のID数 + unknown + パディングID\n",
        "EMB_SIZE = 300                               # 単語埋め込みベクトルのサイズ\n",
        "PADDING_IDX = len(set(word2id.values())) + 1 # パディングID\n",
        "OUTPUT_SIZE = 4                              # 出力サイズ\n",
        "HIDDEN_SIZE = 50                             # 隠れ層のサイズ\n",
        "NUM_LAYERS = 1                               # RNN層の数\n",
        "\n",
        "# モデルの定義\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)  # RNNクラスのインスタンスを作成\n",
        "print(model)                                                                          # モデルの構造を確認"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY0GKwcpKlfN",
        "outputId": "d9c9052d-cc5a-43aa-a769-d3b36140fefb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (emb): Embedding(10328, 300, padding_idx=10327)\n",
            "  (rnn): LSTM(300, 50, batch_first=True)\n",
            "  (fc): Linear(in_features=50, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([tokenizer(sample)], dtype=torch.int64)                           # 文章を単語IDに変換\n",
        "print(x)                                                                           # 文章をIDでを表示\n",
        "print(x.size())                                                                    # 文章のサイズを確認\n",
        "print(nn.Softmax(dim=-1)(model(x)))                                                # 出力を確認"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-8OkqkTLAXg",
        "outputId": "2fdd0d13-48fe-47a0-b198-49cd2008d228"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  66,   79,  733, 2094,   21, 4933, 6674,   35, 1514,   86,    0]])\n",
            "torch.Size([1, 11])\n",
            "tensor([[0.2148, 0.2503, 0.2325, 0.3023]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ターゲットのテンソル化\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "Y_train = torch.from_numpy(train['CATEGORY'].map(category_dict).values)\n",
        "Y_valid = torch.from_numpy(valid['CATEGORY'].map(category_dict).values)\n",
        "Y_test = torch.from_numpy(test['CATEGORY'].map(category_dict).values)\n",
        "print(Y_train.size())\n",
        "print(Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhA2l-VwAsr_",
        "outputId": "dc3cf6e1-0e3f-4c84-9029-d326938ffdfa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10672])\n",
            "tensor([2, 0, 2,  ..., 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    newsのDatasetクラス\n",
        "\n",
        "    Attributes\n",
        "    ----------------------------\n",
        "    X : データフレーム\n",
        "        単語ベクトルの平均をまとめたテンソル\n",
        "    y : テンソル\n",
        "        カテゴリをラベル化したテンソル\n",
        "    phase : 'train' or 'val'\n",
        "        学習か訓練かを設定する\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, phase='train'):\n",
        "        self.X = X['TITLE']\n",
        "        self.y = y\n",
        "        self.phase = phase\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"全データサイズを返す\"\"\"\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"idxに対応するテンソル形式のデータとラベルを取得\"\"\"\n",
        "        inputs = torch.tensor(tokenizer(self.X[idx]))\n",
        "        return inputs, self.y[idx]\n",
        "\n",
        "train_dataset = NewsDataset(train, Y_train, phase='train')\n",
        "valid_dataset = NewsDataset(valid, Y_valid, phase='val')\n",
        "test_dataset = NewsDataset(test, Y_test, phase='val')\n",
        "# 動作確認\n",
        "idx = 0\n",
        "print(train_dataset.__getitem__(idx)[0].size())\n",
        "print(train_dataset.__getitem__(idx)[1])\n",
        "print(valid_dataset.__getitem__(idx)[0].size())\n",
        "print(valid_dataset.__getitem__(idx)[1])\n",
        "print(test_dataset.__getitem__(idx)[0].size())\n",
        "print(test_dataset.__getitem__(idx)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMifdMwaAvji",
        "outputId": "9ad03a20-0600-46e1-8bad-a954411534f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11])\n",
            "tensor(2)\n",
            "torch.Size([11])\n",
            "tensor(3)\n",
            "torch.Size([13])\n",
            "tensor(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoaderを作成\n",
        "batch_size = 1\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
        "valid_dataloader = data.DataLoader(\n",
        "            valid_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)\n",
        "test_dataloader = data.DataLoader(\n",
        "            test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "dataloaders_dict = {'train': train_dataloader,\n",
        "                    'val': valid_dataloader,\n",
        "                    'test': test_dataloader,\n",
        "                   }\n",
        "\n",
        "# 動作確認\n",
        "batch_iter = iter(dataloaders_dict['train'])\n",
        "inputs, labels = next(batch_iter)\n",
        "print(inputs.size())\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRSh5D22Axsg",
        "outputId": "bfb04a1a-064e-47f8-bab4-1fe7b4071847"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 12])\n",
            "tensor([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 学習用の関数を定義\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    valid_loss = []\n",
        "    valid_acc = []\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {} / {}'.format(epoch + 1, num_epochs))\n",
        "        print('--------------------------------------------')\n",
        "\n",
        "        # epochごとの学習と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train() # 訓練モード\n",
        "            else:\n",
        "                net.eval() # 検証モード\n",
        "\n",
        "            epoch_loss = 0.0 # epochの損失和\n",
        "            epoch_corrects = 0 # epochの正解数\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for inputs, labels in tqdm(dataloaders_dict[phase]):\n",
        "                optimizer.zero_grad() # optimizerを初期化\n",
        "\n",
        "                # 順伝播計算(forward)\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(inputs)\n",
        "                    loss = criterion(outputs, labels) # 損失を計算\n",
        "                    _, preds = torch.max(outputs, 1) # ラベルを予想\n",
        "\n",
        "                    # 訓練時は逆伝播\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # イテレーション結果の計算\n",
        "                    # lossの合計を更新\n",
        "                    epoch_loss += loss.item() * inputs.size(0)\n",
        "                    # 正解数の合計を更新\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率の表示\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "                train_acc.append(epoch_acc)\n",
        "            else:\n",
        "                valid_loss.append(epoch_loss)\n",
        "                valid_acc.append(epoch_acc)\n",
        "\n",
        "            print('{} Loss: {:.4f}, Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "    return train_loss, train_acc, valid_loss, valid_acc\n",
        "\n",
        "# 学習を実行する\n",
        "\n",
        "# モデルの定義\n",
        "net = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)\n",
        "net.train()\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 最適化手法の定義\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "num_epochs = 5\n",
        "train_loss, train_acc, valid_loss, valid_acc = train_model(net,\n",
        "            dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdVeTIkeA0eZ",
        "outputId": "1b3c16fc-b358-4d2d-a2ec-44651d47f6df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 5\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10672/10672 [04:45<00:00, 37.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.9923, Acc: 0.6209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [00:01<00:00, 1234.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.8070, Acc: 0.7069\n",
            "Epoch 2 / 5\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10672/10672 [06:09<00:00, 28.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.6632, Acc: 0.7586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [00:01<00:00, 1264.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6239, Acc: 0.7691\n",
            "Epoch 3 / 5\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10672/10672 [06:03<00:00, 29.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.4446, Acc: 0.8401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [00:01<00:00, 1281.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6056, Acc: 0.7864\n",
            "Epoch 4 / 5\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10672/10672 [05:58<00:00, 29.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.2797, Acc: 0.9045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [00:01<00:00, 1256.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6004, Acc: 0.7826\n",
            "Epoch 5 / 5\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10672/10672 [06:10<00:00, 28.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 0.1649, Acc: 0.9457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1334/1334 [00:01<00:00, 920.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.5966, Acc: 0.8028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3W_cKr_XQdh",
        "outputId": "d02215f7-db8b-4e71-be71-3221f510ef62"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchinfo"
      ],
      "metadata": {
        "id": "GJp3iMQQKqVL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
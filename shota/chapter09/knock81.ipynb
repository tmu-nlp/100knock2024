{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3LQlfugXBRi",
        "outputId": "451a9d8c-a4eb-455f-ee31-b6391a66484d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "学習データ\n",
            "CATEGORY\n",
            "b    4502\n",
            "e    4223\n",
            "t    1219\n",
            "m     728\n",
            "Name: count, dtype: int64\n",
            "検証データ\n",
            "CATEGORY\n",
            "b    562\n",
            "e    528\n",
            "t    153\n",
            "m     91\n",
            "Name: count, dtype: int64\n",
            "評価データ\n",
            "CATEGORY\n",
            "b    563\n",
            "e    528\n",
            "t    152\n",
            "m     91\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#データの読み込み\n",
        "import pandas as pd\n",
        "train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/chapter09/train.txt', sep=\"\\t\")\n",
        "test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/chapter09/test.txt', sep=\"\\t\")\n",
        "valid = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/chapter09/valid.txt', sep=\"\\t\")\n",
        "# データ数の確認\n",
        "print('学習データ')\n",
        "print(train['CATEGORY'].value_counts())\n",
        "print('検証データ')\n",
        "print(valid['CATEGORY'].value_counts())\n",
        "print('評価データ')\n",
        "print(test['CATEGORY'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 単語の辞書を作成\n",
        "from collections import Counter\n",
        "words = []\n",
        "for text in train['TITLE']:                #訓練データから文章を1つずつ取り出す\n",
        "    for word in text.rstrip().split():     #文章を単語に分解\n",
        "        words.append(word)                 #単語をリストに追加\n",
        "c = Counter(words)                         #単語の出現回数を数える\n",
        "print(c.most_common(10))                   #頻度上位10単語\n",
        "word2id = {}                               #単語IDの辞書\n",
        "for i, cnt in enumerate(c.most_common()):  #頻度上位10単語分繰り返す\n",
        "    if cnt[1] > 1:                         #出現回数が1より大きい単語のみ\n",
        "        word2id[cnt[0]] = i + 1            #辞書に単語とIDを紐付ける\n",
        "for i, cnt in enumerate(word2id.items()):  #辞書の中身を確認\n",
        "    if i >= 10:                            #10単語だけ表示\n",
        "        break                              #for文を抜ける\n",
        "    print(cnt[0], cnt[1])                  #単語とIDを表示"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaoFH2FcaZcJ",
        "outputId": "c70dc32d-ed8e-4a17-c679-e6878a5cc904"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('to', 2151), ('...', 2031), ('in', 1415), ('as', 1027), ('on', 1025), ('UPDATE', 1000), ('-', 991), ('for', 969), ('of', 957), ('The', 859)]\n",
            "to 1\n",
            "... 2\n",
            "in 3\n",
            "as 4\n",
            "on 5\n",
            "UPDATE 6\n",
            "- 7\n",
            "for 8\n",
            "of 9\n",
            "The 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 単語のID化\n",
        "def tokenizer(text):                                 #単語IDのリストを返す関数\n",
        "    words = text.rstrip().split()                    #単語に分解\n",
        "    return [word2id.get(word, 0) for word in words]  #単語のIDに変換\n",
        "\n",
        "sample = train.at[0, 'TITLE']                        #学習データの1つ目の文章\n",
        "print(sample)                                        #文章を表示\n",
        "print(tokenizer(sample))                             #文章を単語IDに変換"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-hhnRWjagOZ",
        "outputId": "a50fd0e0-c49a-46b2-9c5f-f619baa18ff8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justin Bieber Under Investigation For Attempted Robbery At Dave & Buster's\n",
            "[66, 79, 733, 2094, 21, 4933, 6674, 35, 1514, 86, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNNの作成\n",
        "# モデルの構築\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as data\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "\n",
        "# 乱数のシードを設定\n",
        "# parserなどで指定\n",
        "seed = 1234\n",
        "\n",
        "random.seed(seed)                                   # Python標準ライブラリの乱数のシードを設定\n",
        "np.random.seed(seed)                                # Numpy乱数のシードを設定\n",
        "torch.manual_seed(seed)                             # PyTorch乱数のシードを設定\n",
        "torch.cuda.manual_seed(seed)                        # PyTorchのCUDA乱数のシードを設定\n",
        "torch.backends.cudnn.benchmark = False              # PyTorchのCUDNNのベンチマークを使用しない  (cudnn内の非決定的な処理の固定化)\n",
        "torch.backends.cudnn.deterministic = True           # PyTorchのCUDNNの定着を使用\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32      # 乱数生成のシードの初期値を設定\n",
        "    np.random.seed(worker_seed)                     # Numpy乱数のシードを設定\n",
        "    random.seed(worker_seed)                        # Python標準ライブラリの乱数のシードを設定\n",
        "\n",
        "g = torch.Generator()                               # PyTorch乱数のシードを設定\n",
        "g.manual_seed(seed)                                 # 乱数生成器にシードを設定\n",
        "\n",
        "class RNN(nn.Module):                                                                               # RNNクラスを定義\n",
        "    def __init__(self, vocab_size, emb_size, padding_idx, hidden_size, output_size, num_layers=1):  # コンストラクタ\n",
        "        super().__init__()                                                                          # 親クラスのコンストラクタを呼ぶ\n",
        "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)                      # 単語埋め込み層\n",
        "        self.rnn = nn.LSTM(emb_size, hidden_size, batch_first=True)                                 # RNN層\n",
        "        self.fc = nn.Linear(hidden_size, output_size)                                               # 全結合層\n",
        "\n",
        "    def forward(self, x, h0=None):                                                                  # 順伝播処理\n",
        "        x = self.emb(x)                                                                             # 単語埋め込み\n",
        "        x, h = self.rnn(x, h0)                                                                      # RNN\n",
        "        x = x[:, -1, :]                                                                             # 最後のステップのみを抽出\n",
        "        logits = self.fc(x)                                                                         # 全結合層\n",
        "        return logits                                                                               # 出力\n",
        "\n",
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(word2id.values())) + 2  # 辞書のID数 + unknown + パディングID\n",
        "EMB_SIZE = 300                               # 単語埋め込みベクトルのサイズ\n",
        "PADDING_IDX = len(set(word2id.values())) + 1 # パディングID\n",
        "OUTPUT_SIZE = 4                              # 出力サイズ\n",
        "HIDDEN_SIZE = 50                             # 隠れ層のサイズ\n",
        "NUM_LAYERS = 1                               # RNN層の数\n",
        "\n",
        "# モデルの定義\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)  # RNNクラスのインスタンスを作成\n",
        "print(model)                                                                          # モデルの構造を確認"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY0GKwcpKlfN",
        "outputId": "6f73b1ff-b495-4b62-d5a7-8ad8faad0768"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (emb): Embedding(10328, 300, padding_idx=10327)\n",
            "  (rnn): LSTM(300, 50, batch_first=True)\n",
            "  (fc): Linear(in_features=50, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([tokenizer(sample)], dtype=torch.int64)                           # 文章を単語IDに変換\n",
        "print(x)                                                                           # 文章をIDでを表示\n",
        "print(x.size())                                                                    # 文章のサイズを確認\n",
        "print(nn.Softmax(dim=-1)(model(x)))                                                # 出力を確認"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-8OkqkTLAXg",
        "outputId": "cd925deb-8071-43c7-afcd-794852e7ac5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  66,   79,  733, 2094,   21, 4933, 6674,   35, 1514,   86,    0]])\n",
            "torch.Size([1, 11])\n",
            "tensor([[0.2148, 0.2503, 0.2325, 0.3023]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3W_cKr_XQdh",
        "outputId": "7dd72b6b-3dd6-4b9d-f314-aa2f38367e8e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJp3iMQQKqVL",
        "outputId": "27a92caa-dffe-4d5e-c62a-af0e8e185bda"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ]
    }
  ]
}
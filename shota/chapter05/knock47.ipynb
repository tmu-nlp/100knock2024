{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行動を代わる\tに\t人間に\n",
      "記述をする\tと\t主体と\n",
      "注目を集める\tが\tサポートベクターマシンが\n",
      "経験を行う\tに を\t元に 学習を\n",
      "推論学習をする\tて で に は を を通して\tなされて ACTRで 元に は ルールを 生成規則を通して\n",
      "活躍生成進化を見せる\tて において は\t加えて 技術において 敵対的生成ネットワークは\n",
      "製作開発を行う\tは\tエイダ・ラブレスは\n",
      "処理を行う\tに\tWebに\n",
      "意味をする\tに\tデータに\n",
      "付加処理を行う\tて に\tして コンピュータに\n",
      "研究を進める\tて\t費やして\n",
      "命令をする\tで\t機構で\n",
      "運転をする\tに\t元に\n",
      "特許をする\tが に まで\t日本が に 2018年まで\n",
      "運転をする\tて に\t基づいて 柔軟に\n",
      "注目を集める\tから は\tことから ファジィは\n",
      "制御を用いる\tて も\t受けて 他社も\n",
      "制御をする\tから\t少なさから\n",
      "改善を果たす\tが で に\tチームが 画像処理コンテストで 2012年に\n",
      "研究を続ける\tが て\tジェフ・ホーキンスが 向けて\n",
      "注目を集める\tに\t急速に\n",
      "主導投資を行う\tで に\t民間企業で 全世界的に\n",
      "実装探索を行う\tで\t無報酬で\n",
      "推論をする\tて\t経て\n",
      "共同研究を始める\tとも は\tマックスプランク研究所とも Googleは\n",
      "研究を行う\tて\t始めて\n",
      "投資をする\tに は まで\tに 韓国は 2022年まで\n",
      "反乱を起こす\tに対して\t人間に対して\n",
      "監視を行う\tに まで\t人工知能に 歩行者まで\n",
      "判断を介す\tから\t観点から\n",
      "禁止を求める\tが に は\tヒューマン・ライツ・ウォッチが 4月に は\n",
      "開発競争を行う\tは をめぐって\t米国中国ロシアは 軍事利用をめぐって\n",
      "暴露拒否追及を受ける\tて で と とともに は\tされて 整合性で すると とともに は\n",
      "共同研究をする\tが\tMicrosoftが\n",
      "発表解任をする\tて は\t含まれて Google社員らは\n",
      "要請解散をする\tが で は\t倫理委員会が 理由で Googleは\n",
      "存在を見いだす\tに\tものに\n",
      "話をする\tは\t哲学者は\n",
      "議論を行う\tまで\tこれまで\n"
     ]
    }
   ],
   "source": [
    "#41で定義した関数を使用するためにimport\n",
    "from importnb import imports\n",
    "with imports('ipynb'):\n",
    "    import knock41 \n",
    "sentences = knock41.morphs_chunk()\n",
    "\n",
    "def mining(sentence):\n",
    "    particles = []\n",
    "    for chunk in sentence:                                       #文から文節を取り出す\n",
    "        predicate = ''                                           #動詞にかかる「サ変接続名詞＋を」\n",
    "        for morph in chunk.morphs:                               #文節から単語を取り出す\n",
    "            if morph.pos == '動詞':                              #単語の品詞が動詞の時\n",
    "                if len(chunk.srcs) != 0:                         #その動詞の係り元の有無\n",
    "                    modifier = []                                #係り元の格リスト\n",
    "                    mod_dic = {}                                 #かかっている文節用辞書\n",
    "                    particle = ''                                #かかっている文節用\n",
    "                    flg2  = 0                                    #「を(助詞)」を判定するフラグ\n",
    "                    for src in chunk.srcs:                       #係り元について\n",
    "                        flg = 0                                  #サ変接続名詞の登場を判定するフラグ\n",
    "                        for x in sentence[src].morphs:           #係り元の文節内の単語\n",
    "                            if x.pos == '名詞' and x.pos1 == 'サ変接続' and flg2 == 0:                  #取り出した単語が、サ変接続名詞であるとき\n",
    "                                predicate += x.surface                                                 #それをpredicateに追加\n",
    "                                flg = 1                                                                #flgをたてる\n",
    "                                continue\n",
    "                            elif x.pos == '助詞' and x.surface == 'を' and flg == 1 and flg2 == 0:     #取り出した単語が、助詞のをであるとき\n",
    "                                predicate += 'を'                                                      #それをpredicateに追加\n",
    "                                flg = 0                                                                #flgを下す\n",
    "                                flg2  = 1                                                              #flg2をたてる\n",
    "                                continue\n",
    "                            elif x.pos != '記号':                  #記号でないとき\n",
    "                                particle += x.surface            #それを文節に追加\n",
    "\n",
    "                            if x.pos == '助詞':                  #助詞の時\n",
    "                                modifier.append(x.surface)       #係り元の格リストに追加\n",
    "                                mod_dic[x.surface] = particle    #格をキーとして文節を辞書に格納                                \n",
    "                                particle = ''                    #かかっている文節をリセット\n",
    "                            else:\n",
    "                                flg = 0\n",
    "                        particle = ''\n",
    "                    if len(modifier) != 0 and flg2 == 1:                #係り元の格が存在すれば\n",
    "                        modifier = sorted(list(set(modifier)))\n",
    "                        particles.append(predicate + morph.base + '\\t' + ' '.join(modifier) + '\\t' + ' '.join([mod_dic[x] for x in modifier]))  #それを出力用の配列に追加\n",
    "                break\n",
    "    return particles\n",
    "\n",
    "for sentence in sentences:\n",
    "    particles = mining(sentence)\n",
    "    for particle in particles:\n",
    "        print(particle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

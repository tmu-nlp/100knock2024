{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bVZdg7QScSzQ"
      },
      "outputs": [],
      "source": [
        "#作成したテンソルを読み込み\n",
        "import torch\n",
        "X_train = torch.load('/content/drive/MyDrive/Colab Notebooks/chapter08/X_train.pt')\n",
        "X_test = torch.load('/content/drive/MyDrive/Colab Notebooks/chapter08/X_test.pt')\n",
        "X_valid = torch.load('/content/drive/MyDrive/Colab Notebooks/chapter08/X_valid.pt')\n",
        "Y_train = torch.load('/content/drive/MyDrive/Colab Notebooks/chapter08/Y_train.pt')\n",
        "Y_test = torch.load('/content/drive/MyDrive/Colab Notebooks/chapter08/Y_test.pt')\n",
        "Y_valid = torch.load('/content/drive/MyDrive/Colab Notebooks/chapter08/Y_valid.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oHlM8QkcYY8",
        "outputId": "f17c5304-0f70-46f0-d98a-99fde19c8ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SLNet(\n",
            "  (fc): Linear(in_features=300, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# モデルの構築(nnはニューラルネットワーク)\n",
        "from torch import nn\n",
        "\n",
        "class SLNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size) #全結合層の作成\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.fc(x)                          #フォワードパスの作成\n",
        "        return logits\n",
        "\n",
        "#入力300,出力4のサイズでインスタンスを作成\n",
        "model = SLNet(300, 4)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEv7dFgGciXk",
        "outputId": "c99cbddf-65d6-4b92-ec60-b5e9fe781754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([300])\n",
            "tensor(2)\n",
            "torch.Size([300])\n",
            "tensor(3)\n",
            "torch.Size([300])\n",
            "tensor(2)\n"
          ]
        }
      ],
      "source": [
        "# データセットを作成する\n",
        "import torch.utils.data as data\n",
        "\n",
        "#データセットのクラスを作成\n",
        "class NewsDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    newsのDatasetクラス\n",
        "\n",
        "    Attributes\n",
        "    ----------------------------\n",
        "    X : テンソル\n",
        "        単語ベクトルの平均をまとめたテンソル\n",
        "    y : テンソル\n",
        "        カテゴリをラベル化したテンソル\n",
        "    phase : 'train' or 'val'\n",
        "        学習か訓練かを設定する\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, phase='train'):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.phase = phase\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"全データサイズを返す\"\"\"\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"idxに対応するテンソル形式のデータとラベルを取得\"\"\"\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = NewsDataset(X_train, Y_train, phase='train')\n",
        "valid_dataset = NewsDataset(X_valid, Y_valid, phase='val')\n",
        "test_dataset = NewsDataset(X_test, Y_test, phase='val')\n",
        "\n",
        "# 動作確認\n",
        "idx = 0\n",
        "print(train_dataset.__getitem__(idx)[0].size())\n",
        "print(train_dataset.__getitem__(idx)[1])\n",
        "print(valid_dataset.__getitem__(idx)[0].size())\n",
        "print(valid_dataset.__getitem__(idx)[1])\n",
        "print(test_dataset.__getitem__(idx)[0].size())\n",
        "print(test_dataset.__getitem__(idx)[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV2DBdYydYE3",
        "outputId": "394616c2-b647-4d20-be23-5756fd0e9291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 300])\n",
            "tensor([2])\n"
          ]
        }
      ],
      "source": [
        "# DataLoaderを作成(Dataloaderはdatasetsからバッチごとに取り出すことを目的とする)\n",
        "batch_size = 1\n",
        "\n",
        "train_dataloader = data.DataLoader(                                              #ここには10671個のバッチが含まれている\n",
        "            train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = data.DataLoader(\n",
        "            valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
        "test_dataloader = data.DataLoader(\n",
        "            test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "\n",
        "dataloaders_dict = {'train': train_dataloader,\n",
        "                    'val': valid_dataloader,\n",
        "                    'test': test_dataloader,\n",
        "                   }\n",
        "\n",
        "# 動作確認\n",
        "batch_iter = iter(dataloaders_dict['train']) #バッチからデータを取り出す\n",
        "inputs, labels = next(batch_iter)\n",
        "print(inputs.size())\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HJazYT0dgpt",
        "outputId": "6e9b845b-0719-4b6e-df51-8205a62dfba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_size: 1\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10672/10672 [00:07<00:00, 1348.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.4156, Acc: 0.8604\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 30.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.3103, Acc: 0.9048\n",
            "batch_size 1 calc_time: 7.9691 sec\n",
            "batch_size: 2\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5336/5336 [00:05<00:00, 1000.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.4747, Acc: 0.8406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 60.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.3473, Acc: 0.9003\n",
            "batch_size 2 calc_time: 5.3695 sec\n",
            "batch_size: 4\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2668/2668 [00:02<00:00, 1250.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.5581, Acc: 0.8145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 33.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.4077, Acc: 0.8771\n",
            "batch_size 4 calc_time: 2.1905 sec\n",
            "batch_size: 8\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1334/1334 [00:01<00:00, 1177.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.6571, Acc: 0.7880\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 53.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.4918, Acc: 0.8343\n",
            "batch_size 8 calc_time: 1.1715 sec\n",
            "batch_size: 16\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 667/667 [00:00<00:00, 1011.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.7672, Acc: 0.7598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 54.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.5949, Acc: 0.7954\n",
            "batch_size 16 calc_time: 0.6976 sec\n",
            "batch_size: 32\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 334/334 [00:00<00:00, 723.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.8835, Acc: 0.7550\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 27.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.7090, Acc: 0.7864\n",
            "batch_size 32 calc_time: 0.5321 sec\n",
            "batch_size: 64\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 167/167 [00:00<00:00, 359.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 1.0068, Acc: 0.7340\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 34.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.8364, Acc: 0.7819\n",
            "batch_size 64 calc_time: 0.5246 sec\n",
            "batch_size: 128\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84/84 [00:00<00:00, 272.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 1.1195, Acc: 0.6656\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 41.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.9572, Acc: 0.7789\n",
            "batch_size 128 calc_time: 0.3554 sec\n",
            "batch_size: 256\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [00:00<00:00, 129.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 1.2076, Acc: 0.6218\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 30.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 1.0640, Acc: 0.7586\n",
            "batch_size 256 calc_time: 0.3806 sec\n",
            "batch_size: 512\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:00<00:00, 80.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 1.3097, Acc: 0.4804\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 27.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 1.1936, Acc: 0.6799\n",
            "batch_size 512 calc_time: 0.3191 sec\n",
            "batch_size: 1024\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:00<00:00, 49.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 1.3510, Acc: 0.3485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 24.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 1.2931, Acc: 0.5712\n",
            "batch_size 1024 calc_time: 0.2834 sec\n",
            "batch_size: 2048\n",
            "Epoch 1 / 1\n",
            "--------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 10.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 1.3812, Acc: 0.2256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 40.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 1.3537, Acc: 0.4318\n",
            "batch_size 2048 calc_time: 0.6069 sec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# 学習用の関数を定義\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    valid_loss = []\n",
        "    valid_acc = []\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # 開始時刻の記録\n",
        "        start = time.time()\n",
        "        print('Epoch {} / {}'.format(epoch + 1, num_epochs))\n",
        "        print('--------------------------------------------')\n",
        "\n",
        "        # epochごとの学習と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train() # 訓練モード\n",
        "            else:\n",
        "                net.eval() # 検証モード\n",
        "\n",
        "            epoch_loss = 0.0 # epochの損失和\n",
        "            epoch_corrects = 0 # epochの正解数\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for inputs, labels in tqdm(dataloaders_dict[phase]):\n",
        "                optimizer.zero_grad() # optimizerを初期化\n",
        "\n",
        "                # 順伝播計算(forward)\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(inputs)\n",
        "                    loss = criterion(outputs, labels) # 損失を計算\n",
        "                    _, preds = torch.max(outputs, 1) # ラベルを予想\n",
        "\n",
        "                    # 訓練時は逆伝播\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # イテレーション結果の計算\n",
        "                    # lossの合計を更新\n",
        "                    epoch_loss += loss.item() * inputs.size(0)\n",
        "                    # 正解数の合計を更新\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率の表示\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "                train_acc.append(epoch_acc)\n",
        "            else:\n",
        "                valid_loss.append(epoch_loss)\n",
        "                valid_acc.append(epoch_acc)\n",
        "\n",
        "            print('{} Loss: {:.4f}, Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "        # 修了時刻の記録\n",
        "        end = time.time()\n",
        "        calc_time = end - start\n",
        "        print('batch_size {} calc_time: {:.4f} sec'.format(batch_size, calc_time))\n",
        "    return train_loss, train_acc, valid_loss, valid_acc, calc_time\n",
        "\n",
        "\n",
        "# 学習を実行する\n",
        "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]                           #今回検証するバッチサイズを保持\n",
        "cpu_times = []                                                                              #処理時間を保存するための配列\n",
        "for batch_size in batch_sizes:                                                              #バッチサイズによってループ\n",
        "    print('batch_size: {}'.format(batch_size))\n",
        "    # DataLoaderを作成\n",
        "    train_dataloader = data.DataLoader(\n",
        "                train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    valid_dataloader = data.DataLoader(\n",
        "                valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
        "    test_dataloader = data.DataLoader(\n",
        "                test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
        "\n",
        "    dataloaders_dict = {'train': train_dataloader,\n",
        "                        'val': valid_dataloader,\n",
        "                        'test': test_dataloader,\n",
        "                       }\n",
        "    # モデルの定義\n",
        "    net = SLNet(300, 4)\n",
        "    net.train()\n",
        "\n",
        "    # 損失関数の定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 最適化手法の定義\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    num_epochs = 1\n",
        "    train_loss, train_acc, valid_loss, valid_acc, calc_time = \\\n",
        "                        train_model(net, dataloaders_dict, criterion, optimizer,\n",
        "                                    num_epochs=num_epochs)\n",
        "    cpu_times.append(calc_time)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

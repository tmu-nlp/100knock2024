{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>RNN(=再帰型ニューラルネットワーク)</u>   \n",
    "ディープラーニングの構造．過去の情報を利用して現在，及び将来の入力に対するネットワークの性能を向上させる．    \n",
    "<u>one-hot表現</u>    \n",
    "単語とone-hotベクトルが1:1対応している．    \n",
    "<u>ソフトマックス関数</u>    \n",
    "入力されたベクトルの各成分を0.0~1.0の範囲の確率値に変換する関数．   \n",
    "ソフトマックス関数によって出力されるベクトルの各成分の和は1.0(=100%)となる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn  as nn\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from importnb import imports\n",
    "with imports (\"ipynb\"):\n",
    "    import  knock80 as k80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, emb_size, pad_idx, output_size):\n",
    "        super().__init__()\n",
    "        self.hid_size = hidden_size# 隠れ状態の特徴量ベクトルの数\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "        \"\"\"\n",
    "        nn.Enbedding\n",
    "        処理しやすい数値ベクトル(=埋め込みベクトル)に変換してあげる\n",
    "        入力: {'this': 0, 'is': 1, 'a': 2, 'sentence': 3}\n",
    "                だとすると[0, 1, 2, 3]みたいな単語IDの並び\n",
    "        padding_idx: ベクトルを計算したくない部分のインデックス\n",
    "        出力: [[単語ID 0 に対する埋め込みベクトル],\n",
    "                [単語ID 1 に対する埋め込みベクトル],\n",
    "                [単語ID 2 に対する埋め込みベクトル],\n",
    "                [単語ID 3 に対する埋め込みベクトル]]\n",
    "        \"\"\"\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, \n",
    "                          nonlinearity=\"tanh\", batch_first=True)\n",
    "        \"\"\"\n",
    "        nonlinearity: 使用する非線形の関数\n",
    "        tanh: ハイパボリックタンジェント．あらゆる入力値を-1.0~1.0の範囲に変換する．\n",
    "        batch_first: Trueの場合，入力や出力の際にbatchが先頭に来る．\n",
    "        \"\"\"\n",
    "        self.fc = nn.Linear(hidden_size, output_size)# 線形変換\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.batch_size = x.size()[0]# size():全要素数\n",
    "        hidden = torch.zeros(1, self.batch_size, self.hid_size)#スカラー変換\n",
    "        emb = self.emb(x)#埋め込みベクトルへの変換\n",
    "        out, hidden = self.rnn(emb, hidden)#rnnを通して計算\n",
    "        out = self.fc(out[:, -1, :])#最後の層を出力\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, x, y, tokenizer, word_dict):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer#文字列を数値変換する\n",
    "        self.word_dict = word_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.x[idx]\n",
    "        inputs = self.tokenizer(self.word_dict, text)\n",
    "\n",
    "        #torch.tensor: tensor型に変換する\n",
    "        return {\n",
    "            \"inputs\": torch.tensor(inputs, dtype=torch.int64),\n",
    "            \"labels\": torch.tensor(self.y[idx], dtype=torch.int64)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../chapter06/train.txt\", sep=\"\\t\")\n",
    "valid = pd.read_csv(\"../chapter06/valid.txt\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"../chapter06/test.txt\", sep=\"\\t\")\n",
    "\n",
    "category = {\"b\":0, \"t\":1, \"e\":2, \"m\":3}\n",
    "y_train = torch.tensor(train[\"CATEGORY\"].map(lambda x: category[x]).values)\n",
    "y_valid = torch.tensor(valid[\"CATEGORY\"].map(lambda x: category[x]).values)\n",
    "y_test = torch.tensor(test[\"CATEGORY\"].map(lambda x: category[x]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = NewsDataset(train[\"TITLE\"], y_train, k80.find_id_by_word, k80.train_id)\n",
    "dataset_valid = NewsDataset(valid[\"TITLE\"], y_valid, k80.find_id_by_word, k80.train_id)\n",
    "dataset_test = NewsDataset(test[\"TITLE\"], y_test, k80.find_id_by_word, k80.train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(Dataset): 10672\n",
      "Dataset[index]:\n",
      "inputs: tensor([2037,   16, 2022, 2501, 3538,  166,    5,   64,    0])\n",
      "labels: 0\n",
      "tensor([[0.1702, 0.0962, 0.2774, 0.4563]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1465, 0.0973, 0.2686, 0.4876]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2552, 0.3165, 0.2917, 0.1366]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1755, 0.0849, 0.2442, 0.4954]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5121, 0.1467, 0.1631, 0.1781]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1383, 0.0889, 0.2930, 0.4798]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1622, 0.0996, 0.2630, 0.4752]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2551, 0.2008, 0.2316, 0.3126]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0951, 0.2406, 0.5217, 0.1425]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.2083, 0.1032, 0.2392, 0.4493]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/m23jt2g15kz8d77xv0qpzbt40000gn/T/ipykernel_8652/679476676.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"labels\": torch.tensor(self.y[idx], dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(Dataset): {len(dataset_train)}\")\n",
    "print(\"Dataset[index]:\")\n",
    "for var in dataset_train[1]:\n",
    "    print(f\"{var}: {dataset_train[1][var]}\")\n",
    "\n",
    "VOCAB_SIZE = len(set(k80.train_id.values())) + 1\n",
    "EMB_SIZE = 300\n",
    "PADDING_IDX = len(set(k80.train_id.values()))\n",
    "OUTPUT_SIZE = 4\n",
    "HIDDEN_SIZE = 50\n",
    "\n",
    "model = RNN(HIDDEN_SIZE, VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE)\n",
    "for i in range(10):\n",
    "    X = dataset_train[i][\"inputs\"]\n",
    "    print(torch.softmax(model(X.unsqueeze(0)), dim=-1))\n",
    "    #unsqueeze: 指定した位置にサイズ1の次元を挿入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

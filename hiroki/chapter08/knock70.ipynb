{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMdW6rVGfSXbelOh2UyKGMO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3kfXMyJ0QQbl"},"outputs":[],"source":["# 単語ベクトルのロード\n","from gensim.models import KeyedVectors\n","\n","file = 'GoogleNews-vectors-negative300.bin.gz'\n","model = KeyedVectors.load_word2vec_format(file, binary=True)\n","\n","# データのロード\n","import pandas as pd\n","import re\n","import numpy as np\n","\n","# ファイル読み込み\n","file = 'newsCorpora.csv'\n","data = pd.read_csv(file, encoding='utf-8', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","data = data.replace('\"', \"'\")\n","# 特定のpublisherのみ抽出\n","publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n","data = data.loc[data['PUBLISHER'].isin(publishers), ['TITLE', 'CATEGORY']].reset_index(drop=True)\n","\n","# 学習用、検証用、評価用に分割する\n","from sklearn.model_selection import train_test_split\n","\n","train, valid_test = train_test_split(data, test_size=0.2, shuffle=True, random_state=64, stratify=data['CATEGORY'])\n","valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=64, stratify=valid_test['CATEGORY'])\n","\n","train = train.reset_index(drop=True)\n","valid = valid.reset_index(drop=True)\n","test = test.reset_index(drop=True)\n","\n","# データ数の確認\n","print('学習データ')\n","print(train['CATEGORY'].value_counts())\n","print('検証データ')\n","print(valid['CATEGORY'].value_counts())\n","print('評価データ')\n","print(test['CATEGORY'].value_counts())\n","\n","import re\n","from nltk import stem\n","\n","# データの結合\n","df = pd.concat([train, valid, test], axis=0).reset_index(drop=True)\n","\n","# 前処理\n","def preprocessing(text):\n","    text_clean = re.sub(r'[\\\"\\'.,:;\\(\\)#\\|\\*\\+\\!\\?#$%&/\\]\\[\\{\\}]', '', text)\n","    text_clean = re.sub('[0-9]+', '0', text_clean)\n","    text_clean = re.sub('\\s-\\s', ' ', text_clean)\n","    return text_clean\n","\n","df['TITLE'] = df['TITLE'].apply(preprocessing)\n"]},{"cell_type":"code","source":["import numpy as np\n","# 平均単語ベクトルの取得\n","def w2v(text):\n","    words = text.rstrip().split()\n","    vec = [model[word] for word in words if word in model]\n","    return np.array(sum(vec) / len(vec))\n","\n","vecs = np.array([])\n","for text in df['TITLE']:\n","    if len(vecs) == 0:\n","        vecs = w2v(text)\n","    else:\n","        vecs = np.vstack([vecs, w2v(text)])\n","\n","# 特徴ベクトルのテンソル化\n","import torch\n","\n","# 乱数のシードを設定\n","torch.manual_seed(1234)\n","np.random.seed(1234)\n","\n","X_train = torch.from_numpy(vecs[:len(train), :])\n","X_valid = torch.from_numpy(vecs[len(train):len(train)+ len(valid), :])\n","X_test = torch.from_numpy(vecs[len(train)+ len(valid):, :])\n","print(X_train.size())\n","print(X_train)"],"metadata":{"id":"zZO5uavuQggW"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOP/lasNwvJiH9jkkwSjlYN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"P9mMewxW1Ha2"},"outputs":[],"source":["from gensim.models import KeyedVectors\n","\n","# 学習済みモデルのロード\n","file = 'GoogleNews-vectors-negative300.bin.gz'\n","model = KeyedVectors.load_word2vec_format(file, binary=True)\n","\n","# 学習済み単語ベクトルの取得\n","VOCAB_SIZE = len(set(word2id.values())) + 2\n","EMB_SIZE = 300\n","weights = np.zeros((VOCAB_SIZE, EMB_SIZE))\n","words_in_pretrained = 0\n","for i, word in enumerate(word2id.keys()):\n","    try:\n","        weights[i] = model[word]\n","        words_in_pretrained += 1\n","    except KeyError:\n","        weights[i] = np.random.normal(scale=0.1, size=(EMB_SIZE,))\n","weights = torch.from_numpy(weights.astype((np.float32)))\n","\n","print(f'学習済みベクトル利用単語数: {words_in_pretrained} / {VOCAB_SIZE}')\n","print(weights.size())"]},{"cell_type":"code","source":["class RNN(nn.Module):\n","    def __init__(self, vocab_size, emb_size, padding_idx, hidden_size, output_size, num_layers=1, emb_weights=None):\n","        super().__init__()\n","        if emb_weights != None:\n","            self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n","        else:\n","            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","        self.rnn = nn.LSTM(emb_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, h0=None):\n","        x = self.emb(x)\n","        x, h = self.rnn(x, h0)\n","        x = x[:, -1, :]\n","        logits = self.fc(x)\n","        return logits\n","\n","# パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 2  # 辞書のID数 + unknown + パディングID\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values())) + 1\n","OUTPUT_SIZE = 4\n","HIDDEN_SIZE = 50\n","NUM_LAYERS = 1\n","\n","# モデルの定義\n","net = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS, weights)\n","net.train()\n","\n","# 損失関数の定義\n","criterion = nn.CrossEntropyLoss()\n","\n","# 最適化手法の定義\n","optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n","\n","num_epochs = 30\n","train_loss_weights, train_acc_weights, valid_loss_weights, valid_acc_weights = train_model(net,\n","            dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)\n","\n","fig, ax = plt.subplots(1,2, figsize=(10, 5))\n","epochs = np.arange(num_epochs)\n","ax[0].plot(epochs, train_loss_weights, label='train')\n","ax[0].plot(epochs, valid_loss_weights, label='valid')\n","ax[0].set_title('loss')\n","ax[0].set_xlabel('epoch')\n","ax[0].set_ylabel('loss')\n","ax[1].plot(epochs, train_acc_weights, label='train')\n","ax[1].plot(epochs, valid_acc_weights, label='valid')\n","ax[1].set_title('acc')\n","ax[1].set_xlabel('epoch')\n","ax[1].set_ylabel('acc')\n","ax[0].legend(loc='best')\n","ax[1].legend(loc='best')\n","plt.tight_layout()\n","plt.savefig('fig84.png')\n","plt.show()\n","\n","acc_train = calc_acc(net, train_dataloader)\n","acc_valid = calc_acc(net, valid_dataloader)\n","acc_test = calc_acc(net, test_dataloader)\n","print('学習データの正解率: {:.4f}'.format(acc_train))\n","print('検証データの正解率: {:.4f}'.format(acc_valid))\n","print('テストデータの正解率: {:.4f}'.format(acc_test))"],"metadata":{"id":"-R1YJAlWCoMS"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMMosJ29va1XuBDvKHTb+X9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VCCBIuTfC_bm"},"outputs":[],"source":["from torch.nn import functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, kernel_heights, stride, padding, emb_weights=None):\n","        super().__init__()\n","        if emb_weights != None:  # 指定があれば埋め込み層の重みをemb_weightsで初期化\n","            self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n","        else:\n","            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","        self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding, 0))\n","        self.drop = nn.Dropout(0.4)\n","        self.fc = nn.Linear(out_channels, output_size)\n","\n","    def forward(self, x):\n","        emb = self.emb(x).unsqueeze(1)\n","        conv = self.conv(emb)\n","        act = F.relu(conv.squeeze(3))\n","        max_pool = F.max_pool1d(act, act.size()[2])\n","        logits = self.fc(self.drop(max_pool.squeeze(2)))\n","        return logits\n","\n","# パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 2\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values())) + 1\n","OUTPUT_SIZE = 4\n","OUT_CHANNELS = 100\n","KERNEL_HEIGHTS = 3\n","STRIDE = 1\n","PADDING = 1\n","\n","# モデルの定義\n","model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=weights)\n","x = torch.tensor([tokenizer(sample)], dtype=torch.int64)\n","print(x)\n","print(x.size())\n","print(nn.Softmax(dim=-1)(model(x)))"]}]}
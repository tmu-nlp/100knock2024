{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQ7xcleKzymhmnqmZLzOIL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6lKi9b5hyAXZ"},"outputs":[],"source":["# データのロード\n","import pandas as pd\n","import re\n","import numpy as np\n","\n","# ファイル読み込み\n","file = 'newsCorpora.csv'\n","data = pd.read_csv(file, encoding='utf-8', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","data = data.replace('\"', \"'\")\n","# 特定のpublisherのみ抽出\n","publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n","data = data.loc[data['PUBLISHER'].isin(publishers), ['TITLE', 'CATEGORY']].reset_index(drop=True)\n","\n","# 前処理\n","def preprocessing(text):\n","    text_clean = re.sub(r'[\\\"\\'.,:;\\(\\)#\\|\\*\\+\\!\\?#$%&/\\]\\[\\{\\}]', '', text)\n","    text_clean = re.sub('[0-9]+', '0', text_clean)\n","    text_clean = re.sub('\\s-\\s', ' ', text_clean)\n","    return text_clean\n","\n","data['TITLE'] = data['TITLE'].apply(preprocessing)\n","\n","# 学習用、検証用、評価用に分割する\n","from sklearn.model_selection import train_test_split\n","\n","train, valid_test = train_test_split(data, test_size=0.2, shuffle=True, random_state=64, stratify=data['CATEGORY'])\n","valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=64, stratify=valid_test['CATEGORY'])\n","\n","train = train.reset_index(drop=True)\n","valid = valid.reset_index(drop=True)\n","test = test.reset_index(drop=True)\n","\n","# データ数の確認\n","print('学習データ')\n","print(train['CATEGORY'].value_counts())\n","print('検証データ')\n","print(valid['CATEGORY'].value_counts())\n","print('評価データ')\n","print(test['CATEGORY'].value_counts()"]},{"cell_type":"code","source":["# 単語の辞書を作成\n","from collections import Counter\n","words = []\n","for text in train['TITLE']:\n","    for word in text.rstrip().split():\n","        words.append(word)\n","c = Counter(words)\n","word2id = {}\n","for i, cnt in enumerate(c.most_common()):\n","    if cnt[1] > 1:\n","        word2id[cnt[0]] = i + 1\n","for i, cnt in enumerate(word2id.items()):\n","    if i >= 10:\n","        break\n","    print(cnt[0], cnt[1])"],"metadata":{"id":"OskCc3vMyLD4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 単語のID化\n","def tokenizer(text):\n","    words = text.rstrip().split()\n","    return [word2id.get(word, 0) for word in words]\n","\n","sample = train.at[0, 'TITLE']\n","print(sample)\n","print(tokenizer(sample))"],"metadata":{"id":"UKDxttz9yQyi"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkPf3dXTcaaKtPnePvTXGD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"c3H7zD9dDYFv"},"outputs":[],"source":["# データのロード\n","import pandas as pd\n","import re\n","import numpy as np\n","import random\n","import transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel\n","from torch import optim\n","from torch import cuda\n","from torch import nn\n","from matplotlib import pyplot as plt\n","\n","# 乱数のシードを設定\n","# parserなどで指定\n","seed = 1234\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","g = torch.Generator()\n","g.manual_seed(seed)\n","\n","# ファイル読み込み\n","file = 'newsCorpora.csv'\n","data = pd.read_csv(file, encoding='utf-8', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","data = data.replace('\"', \"'\")\n","# 特定のpublisherのみ抽出\n","publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n","data = data.loc[data['PUBLISHER'].isin(publishers), ['TITLE', 'CATEGORY']].reset_index(drop=True)\n","\n","# 前処理\n","def preprocessing(text):\n","    text_clean = re.sub(r'[\\\"\\'.,:;\\(\\)#\\|\\*\\+\\!\\?#$%&/\\]\\[\\{\\}]', '', text)\n","    text_clean = re.sub('[0-9]+', '0', text_clean)\n","    text_clean = re.sub('\\s-\\s', ' ', text_clean)\n","    return text_clean\n","\n","data['TITLE'] = data['TITLE'].apply(preprocessing)\n","\n","# 学習用、検証用、評価用に分割する\n","from sklearn.model_selection import train_test_split\n","\n","train, valid_test = train_test_split(data, test_size=0.2, shuffle=True, random_state=64, stratify=data['CATEGORY'])\n","valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=64, stratify=valid_test['CATEGORY'])\n","\n","train = train.reset_index(drop=True)\n","valid = valid.reset_index(drop=True)\n","test = test.reset_index(drop=True)\n","\n","# データ数の確認\n","print('学習データ')\n","print(train['CATEGORY'].value_counts())\n","print('検証データ')\n","print(valid['CATEGORY'].value_counts())\n","print('評価データ')\n","print(test['CATEGORY'].value_counts())\n","\n","# ターゲットのテンソル化\n","category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n","Y_train = torch.from_numpy(train['CATEGORY'].map(category_dict).values)\n","Y_valid = torch.from_numpy(valid['CATEGORY'].map(category_dict).values)\n","Y_test = torch.from_numpy(test['CATEGORY'].map(category_dict).values)"]},{"cell_type":"code","source":["class BERTDataSet(Dataset):\n","\n","    def __init__(self, X, y, phase):\n","        self.X = X['TITLE']\n","        self.y = y\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.phase = phase\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self,idx):\n","        sentence = self.X[idx]\n","        sentence = str(sentence)\n","        sentence = \" \".join(sentence.split())\n","\n","        bert_sens = self.tokenizer.encode_plus(\n","                                sentence,\n","                                add_special_tokens = True, # [CLS],[SEP]\n","                                max_length = 20,\n","                                pad_to_max_length = True, # add padding to blank\n","                                truncation=True)\n","\n","        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)\n","        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)\n","        labels = self.y[idx]\n","\n","        return {\n","                'ids': ids,\n","                'mask': mask,\n","                'labels': labels,\n","                }\n","\n","train_dataset = BERTDataSet(train, Y_train, phase='train')\n","valid_dataset = BERTDataSet(valid, Y_valid, phase='val')\n","test_dataset = BERTDataSet(test, Y_test, phase='val')\n","\n","# 動作確認\n","train_dataset[0]"],"metadata":{"id":"IaXcq1lMDgDH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DataLoaderを作成\n","batch_size = 64\n","\n","train_dataloader = DataLoader(\n","            train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n","valid_dataloader = DataLoader(\n","            valid_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","test_dataloader = DataLoader(\n","            test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","\n","dataloaders_dict = {'train': train_dataloader,\n","                    'val': valid_dataloader,\n","                    'test': test_dataloader,\n","                   }\n","\n","# 動作確認\n","batch_iter = iter(dataloaders_dict['train'])\n","inputs = next(batch_iter)\n","print(inputs['ids'].size())\n","print(inputs['mask'].size())\n","print(inputs['labels'].size())"],"metadata":{"id":"7LNogDplDgQX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BERT分類モデルの定義\n","class BERTClass(torch.nn.Module):\n","    def __init__(self, drop_rate, hidden_size, output_size):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.drop = nn.Dropout(drop_rate)\n","        self.fc = nn.Sequential(\n","                                nn.Linear(768, hidden_size),\n","                                nn.ReLU(),\n","                                nn.BatchNorm1d(hidden_size),\n","                                nn.Linear(hidden_size, output_size)\n","                                )\n","        # self.fc = nn.Linear(768, output_size)  # BERTの出力に合わせて768次元を指定\n","\n","    def forward(self, ids, mask):\n","        out = self.bert(ids, attention_mask=mask)[-1]\n","        out = self.fc(self.drop(out))\n","        return out"],"metadata":{"id":"13YUMfRCDoAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 学習用の関数を定義\n","def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n","    # 初期設定\n","    # GPUが使えるか確認\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(torch.cuda.get_device_name())\n","    print(\"使用デバイス:\", device)\n","\n","    # ネットワークをgpuへ\n","    net.to(device)\n","\n","    train_loss = []\n","    train_acc = []\n","    valid_loss = []\n","    valid_acc = []\n","\n","    # epochのループ\n","    for epoch in range(num_epochs):\n","        # epochごとの学習と検証のループ\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                net.train() # 訓練モード\n","            else:\n","                net.eval() # 検証モード\n","\n","            epoch_loss = 0.0 # epochの損失和\n","            epoch_corrects = 0 # epochの正解数\n","\n","            # データローダーからミニバッチを取り出すループ\n","            for data in dataloaders_dict[phase]:\n","                # GPUが使えるならGPUにおくる\n","                ids = data['ids'].to(device)\n","                mask = data['mask'].to(device)\n","                labels = data['labels'].to(device)\n","                optimizer.zero_grad() # optimizerを初期化\n","\n","                # 順伝播計算(forward)\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = net(ids, mask)\n","                    loss = criterion(outputs, labels) # 損失を計算\n","                    _, preds = torch.max(outputs, 1) # ラベルを予想\n","\n","                    # 訓練時は逆伝播\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                    # イテレーション結果の計算\n","                    # lossの合計を更新\n","                    epoch_loss += loss.item() * ids.size(0)\n","                    # 正解数の合計を更新\n","                    epoch_corrects += torch.sum(preds == labels.data)\n","\n","            # epochごとのlossと正解率の表示\n","            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n","            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n","            if phase == 'train':\n","                train_loss.append(epoch_loss)\n","                train_acc.append(epoch_acc.cpu())\n","            else:\n","                valid_loss.append(epoch_loss)\n","                valid_acc.append(epoch_acc.cpu())\n","\n","        print('Epoch {} / {} (train) Loss: {:.4f}, Acc: {:.4f}, (val) Loss: {:.4f}, Acc: {:.4f}'.format(epoch + 1, num_epochs, train_loss[-1], train_acc[-1], valid_loss[-1], valid_acc[-1]))\n","    return train_loss, train_acc, valid_loss, valid_acc\n","\n","\n","# パラメータの設定\n","DROP_RATE = 0.2\n","HIDDEN_SIZE = 256\n","OUTPUT_SIZE = 4\n","BATCH_SIZE = 64\n","NUM_EPOCHS = 8\n","LEARNING_RATE = 1e-5\n","\n","# モデルの定義\n","net = BERTClass(DROP_RATE, HIDDEN_SIZE, OUTPUT_SIZE)\n","net.train()\n","\n","# 損失関数の定義\n","criterion = nn.CrossEntropyLoss()\n","\n","# オプティマイザの定義\n","optimizer = torch.optim.AdamW(params=net.parameters(), lr=LEARNING_RATE)\n","\n","train_loss, train_acc, valid_loss, valid_acc = train_model(net,\n","            dataloaders_dict, criterion, optimizer, num_epochs=NUM_EPOCHS)"],"metadata":{"id":"HVX2USpnDyYq"},"execution_count":null,"outputs":[]}]}
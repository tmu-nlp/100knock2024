{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm","mount_file_id":"1bXrYWkz13xOLvRB4nZMmEHEy7Me3jtuN","authorship_tag":"ABX9TyNYcdJsPMC4kuc1udEneiLT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BXKC_cMywfPa","outputId":"8e1a07ef-b45c-4bee-bf5a-fc74cb8aebfa","executionInfo":{"status":"ok","timestamp":1717596314455,"user_tz":-540,"elapsed":24322,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","--2024-06-05 14:05:04--  https://archive.ics.uci.edu/static/public/359/news+aggregator.zip\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified\n","Saving to: ‘news+aggregator.zip’\n","\n","news+aggregator.zip     [          <=>       ]  27.87M  9.47MB/s    in 2.9s    \n","\n","2024-06-05 14:05:08 (9.47 MB/s) - ‘news+aggregator.zip’ saved [29224203]\n","\n","Archive:  news+aggregator.zip\n","  inflating: 2pageSessions.csv       \n","   creating: __MACOSX/\n","  inflating: __MACOSX/._2pageSessions.csv  \n","  inflating: newsCorpora.csv         \n","  inflating: __MACOSX/._newsCorpora.csv  \n","  inflating: readme.txt              \n","  inflating: __MACOSX/._readme.txt   \n"]}],"source":["#No51(特徴量抽出)\n","#特徴量 対象データの特徴を定量的な数値として表したもの\n","#TfidfVectorizer 単語の出現頻度(TF)と単語が文書にてどのぐらい出てこないか(IDF)で判断する。\n","import pandas as pd\n","import string\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!wget https://archive.ics.uci.edu/static/public/359/news+aggregator.zip\n","!unzip news+aggregator.zip\n","df = pd.read_csv(\"./newsCorpora.csv\",sep=\"\\t\",header=None,names=[\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"])\n","#「PUBLISHER」の行から、”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n","#isinはあくまでbool値を返すことに注意(Trueが抽出される)\n","df = df[df[\"PUBLISHER\"].isin([\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"])]\n","#「TITLE」と「CATEGORY」の列を抽出する．\n","df = df[[\"TITLE\", \"CATEGORY\"]]\n","#学習、検証、評価データに分割する(分割したいもの、割合、shuffleはTrueがデフォルト)\n","#まず0.9:0.1で分ける\n","train, test = train_test_split(df, test_size=0.2)\n","#次に0.2を半分にする(検証、評価データを0.1にする)\n","test, valid = train_test_split(test, test_size=0.5)\n","#学習、検証、評価データをファイルに保存する\n","train.to_csv(\"train.txt\", sep=\"\\t\", index=False, header=None)\n","valid.to_csv(\"valid.txt\", sep=\"\\t\", index=False, header=None)\n","test.to_csv(\"test.txt\", sep=\"\\t\", index=False, header=None)\n","\n"]},{"cell_type":"code","source":["def preprosessing(text):\n","    #string.punctuation 「'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'」のこと\n","    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n","    #maketransで作成したtableを文字列を変換する\n","    text = text.translate(table)\n","    #小文字にする\n","    text = text.lower()\n","    #正規表現を利用するために、compileを用いてpatternを作成する\n","    pattern = re.compile('[0-9]+')\n","    #正規表現にマッチした部分に0を代入\n","    text = re.sub(pattern, '0', text)\n","\n","    return text\n","\n","#データの連結、前処理\n","df = pd.concat([train, valid, test], axis = 0)\n","#もとのindexを削除\n","df.reset_index(drop=True, inplace=True)\n","df['TITLE'] = df['TITLE'].map(lambda x: preprosessing(x)) #map関数を使ってSeriesの各要素に前処理の関数を適用\n","#データの連結、前処理\n","df = pd.concat([train, valid, test], axis = 0)\n","df.reset_index(drop=True, inplace=True)\n","df['TITLE'] = df['TITLE'].map(lambda x: preprosessing(x)) #map関数を使ってSeriesの各要素に前処理の関数を適用\n","\n","#単語のベクトル化\n","vec_tfidf = TfidfVectorizer() #TfidfVectorizerのインスタンス生成\n","data = vec_tfidf.fit_transform(df['TITLE'])\n","data = pd.DataFrame(data.toarray(), columns = vec_tfidf.get_feature_names_out())\n","\n","#整数除算をして、dataを分割する。\n","split_point1 = int(len(data)//3)\n","split_point2 = int(split_point1 * 2)\n","\n","#学習、検証、評価データ\n","x_train = data[:split_point1]\n","x_valid = data[split_point1:split_point2]\n","x_test = data[split_point2:]\n","\n","#学習、検証、評価等別\n","y_data = df['CATEGORY']\n","y_train = y_data[:split_point1]\n","y_valid = y_data[split_point1:split_point2]\n","y_test = y_data[split_point2:]\n","\n","#CSVファイルへの書き出し\n","x_train.to_csv('train.feature.txt', sep = '\\t', index = False)\n","x_valid.to_csv('valid.feature.txt', sep = '\\t', index = False)\n","x_test.to_csv('test.feature.txt', sep = '\\t', index = False)\n"],"metadata":{"id":"HAWCmHJxtp2y","executionInfo":{"status":"ok","timestamp":1717596430656,"user_tz":-540,"elapsed":109201,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}}},"execution_count":2,"outputs":[]}]}
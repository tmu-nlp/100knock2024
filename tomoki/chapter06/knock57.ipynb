{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMTsMiiIELEFoJ3QPLENrTW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"STOfUkx_PwKI"},"outputs":[],"source":["#No57(特徴量の重みの確認)\n","import pandas as pd\n","import string\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!wget https://archive.ics.uci.edu/static/public/359/news+aggregator.zip\n","!unzip news+aggregator.zip\n","df = pd.read_csv(\"./newsCorpora.csv\",sep=\"\\t\",header=None,names=[\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"])\n","#「PUBLISHER」の行から、”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n","#isinはあくまでbool値を返すことに注意(Trueが抽出される)\n","df = df[df[\"PUBLISHER\"].isin([\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"])]\n","#「TITLE」と「CATEGORY」の列を抽出する．\n","df = df[[\"TITLE\", \"CATEGORY\"]]\n","#学習、検証、評価データに分割する(分割したいもの、割合、shuffleはTrueがデフォルト)\n","#まず0.9:0.1で分ける\n","train, test = train_test_split(df, test_size=0.2)\n","#次に0.2を半分にする(検証、評価データを0.1にする)\n","test, valid = train_test_split(test, test_size=0.5)"]},{"cell_type":"code","source":["def preprosessing(text):\n","    #string.punctuation 「'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'」のこと\n","    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n","    #maketransで作成したtableを文字列を変換する\n","    text = text.translate(table)\n","    #小文字にする\n","    text = text.lower()\n","    #正規表現を利用するために、compileを用いてpatternを作成する\n","    pattern = re.compile('[0-9]+')\n","    #正規表現にマッチした部分に0を代入\n","    text = re.sub(pattern, '0', text)\n","\n","    return text\n","\n","#データの連結、前処理\n","df = pd.concat([train, valid, test], axis = 0)\n","#もとのindexを削除\n","df.reset_index(drop=True, inplace=True)\n","\n","df['TITLE'] = df['TITLE'].map(lambda x: preprosessing(x))\n","#データの連結、前処理\n","df = pd.concat([train, valid, test], axis = 0)\n","df.reset_index(drop=True, inplace=True)\n","#map関数　シーケンスの構成要素すべてに対して、ある関数の処理を行わせるという高階関数\n","#lambda関数を用いて、xを引数として、preprosessing関数を呼び出す\n","df['TITLE'] = df['TITLE'].map(lambda x: preprosessing(x))\n","vec_tfidf = TfidfVectorizer() #TfidfVectorizerのインスタンス生成\n","data = vec_tfidf.fit_transform(df['TITLE'])\n","data = pd.DataFrame(data.toarray(), columns = vec_tfidf.get_feature_names_out())\n","#整数除算をして、dataを分割する。\n","split_point1 = int(len(data)//3)\n","split_point2 = int(split_point1 * 2)\n","#学習、検証、評価データ\n","x_train = data[:split_point1]\n","x_valid = data[split_point1:split_point2]\n","x_test = data[split_point2:]\n","#学習、検証、評価等別\n","y_data = df['CATEGORY']\n","y_train = y_data[:split_point1]\n","y_valid = y_data[split_point1:split_point2]\n","y_test = y_data[split_point2:]"],"metadata":{"id":"mCdorhFRQsD0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","#ロジスティック回帰モデルのインスタンス生成\n","model = LogisticRegression()\n","#訓練の実行(学習データ、ラベル(正解))\n","model.fit(x_train, y_train)"],"metadata":{"id":"mrc9zyGkQvsE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 正解率計算用のメソッド\n","from sklearn.metrics import accuracy_score\n","y_pred = model.predict(x_valid)\n","#訓練データとテストデータを用いて予測する\n","y_train_pred = model.predict(x_train)\n","y_test_pred = model.predict(x_test)"],"metadata":{"id":"PMNXOVFRRANW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","features = x_train.columns.values  # 学習データの特徴量\n","#LogisticRegression.classes  分類したカテゴリのラベルを取得する\n","#LogisticRegression.coef     ロジスティック回帰の係数（＝特徴量ごとの重み）を取得する。\n","class_name, coef = model.classes_[0], model.coef_[0]\n","#10位まで取得\n","index = [i for i in range(1, 11)]\n","#任意の行または列を基準にソートする場合は、np.argsort()を使う\n","#afgsortは昇順がデフォルトだが、-を使うことにより、降順にできる。\n","top_10 = features[np.argsort(-coef)[:10]]\n","worst_10 = features[np.argsort(coef)[:10]]\n","\n","df_top_10 = pd.DataFrame(top_10, columns=[f'重みの高い特徴量トップ10（クラス名: {class_name}）'], index = index)\n","df_worst_10 = pd.DataFrame(worst_10, columns=[f'重みの低い特徴量トップ10（クラス名: {class_name}）'], index= index)\n","\n","print(df_top_10)\n","print(df_worst_10)\n","\n"],"metadata":{"id":"8w4qY0h5ULOn"},"execution_count":null,"outputs":[]}]}
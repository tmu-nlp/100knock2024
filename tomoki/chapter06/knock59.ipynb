{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1yz47lSVRxQuaE3V2J2xMgMRn0bASxuKU","timestamp":1717760080734},{"file_id":"1L3vRT96xw8oDMEY7DoBgeaCgNbjQpjfT","timestamp":1717758864601}],"gpuType":"T4","authorship_tag":"ABX9TyNRcwp1U9vo1gRfLYM1oBAy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"STOfUkx_PwKI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717761828920,"user_tz":-540,"elapsed":35236,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}},"outputId":"a2b90462-d86e-4f80-ccce-38383e5a5e2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","--2024-06-07 12:03:40--  https://archive.ics.uci.edu/static/public/359/news+aggregator.zip\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified\n","Saving to: ‘news+aggregator.zip’\n","\n","news+aggregator.zip     [         <=>        ]  27.87M  11.7MB/s    in 2.4s    \n","\n","2024-06-07 12:03:43 (11.7 MB/s) - ‘news+aggregator.zip’ saved [29224203]\n","\n","Archive:  news+aggregator.zip\n","  inflating: 2pageSessions.csv       \n","   creating: __MACOSX/\n","  inflating: __MACOSX/._2pageSessions.csv  \n","  inflating: newsCorpora.csv         \n","  inflating: __MACOSX/._newsCorpora.csv  \n","  inflating: readme.txt              \n","  inflating: __MACOSX/._readme.txt   \n"]}],"source":["#No59(ハイパーパラメータの探索)\n","import pandas as pd\n","import string\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!wget https://archive.ics.uci.edu/static/public/359/news+aggregator.zip\n","!unzip news+aggregator.zip\n","df = pd.read_csv(\"./newsCorpora.csv\",sep=\"\\t\",header=None,names=[\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"])\n","#「PUBLISHER」の行から、”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n","#isinはあくまでbool値を返すことに注意(Trueが抽出される)\n","df = df[df[\"PUBLISHER\"].isin([\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"])]\n","#「TITLE」と「CATEGORY」の列を抽出する．\n","df = df[[\"TITLE\", \"CATEGORY\"]]\n","#学習、検証、評価データに分割する(分割したいもの、割合、shuffleはTrueがデフォルト)\n","#まず0.9:0.1で分ける\n","train, test = train_test_split(df, test_size=0.2)\n","#次に0.2を半分にする(検証、評価データを0.1にする)\n","test, valid = train_test_split(test, test_size=0.5)"]},{"cell_type":"code","source":["def preprosessing(text):\n","    #string.punctuation 「'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'」のこと\n","    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n","    #maketransで作成したtableを文字列を変換する\n","    text = text.translate(table)\n","    #小文字にする\n","    text = text.lower()\n","    #正規表現を利用するために、compileを用いてpatternを作成する\n","    pattern = re.compile('[0-9]+')\n","    #正規表現にマッチした部分に0を代入\n","    text = re.sub(pattern, '0', text)\n","\n","    return text\n","\n","#データの連結、前処理\n","df = pd.concat([train, valid, test], axis = 0)\n","#もとのindexを削除\n","df.reset_index(drop=True, inplace=True)\n","\n","df['TITLE'] = df['TITLE'].map(lambda x: preprosessing(x))\n","#データの連結、前処理\n","df = pd.concat([train, valid, test], axis = 0)\n","df.reset_index(drop=True, inplace=True)\n","#map関数　シーケンスの構成要素すべてに対して、ある関数の処理を行わせるという高階関数\n","#lambda関数を用いて、xを引数として、preprosessing関数を呼び出す\n","df['TITLE'] = df['TITLE'].map(lambda x: preprosessing(x))\n","vec_tfidf = TfidfVectorizer() #TfidfVectorizerのインスタンス生成\n","data = vec_tfidf.fit_transform(df['TITLE'])\n","data = pd.DataFrame(data.toarray(), columns = vec_tfidf.get_feature_names_out())\n","#整数除算をして、dataを分割する。\n","split_point1 = int(len(data)//3)\n","split_point2 = int(split_point1 * 2)\n","#学習、検証、評価データ\n","x_train = data[:split_point1]\n","x_valid = data[split_point1:split_point2]\n","x_test = data[split_point2:]\n","#学習、検証、評価等別\n","y_data = df['CATEGORY']\n","y_train = y_data[:split_point1]\n","y_valid = y_data[split_point1:split_point2]\n","y_test = y_data[split_point2:]"],"metadata":{"id":"mCdorhFRQsD0","executionInfo":{"status":"ok","timestamp":1717761834202,"user_tz":-540,"elapsed":1869,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","#グリッドリサーチを行う\n","#すべての組み合わせを試せる反面、時間や容量を食ってしまうことに注意\n","from sklearn.model_selection import GridSearchCV\n","\n","params = {\"C\": [0.001, 0.005, 10]}\n","\n","# グリッドサーチを行う\n","#学習の最大回数を1500,5分割検証\n","gs_model = GridSearchCV(LogisticRegression(max_iter=1500),params, cv=5, verbose=1)\n","gs_model.fit(x_train, y_train)\n","\n","#最適なモデルを取得する\n","#小数点第2まで表示\n","best_gs_model = gs_model.best_estimator_\n","print(\"\\ntrain_score: {:.2%}\".format(best_gs_model.score(x_train, y_train)))\n","print(\"valid_score: {:.2%}\".format(best_gs_model.score(x_valid, y_valid)))\n","print(\"test_score: {:.2%}\".format(best_gs_model.score(x_test, y_test)))\n","\n","\n","\n"],"metadata":{"id":"mrc9zyGkQvsE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717762117681,"user_tz":-540,"elapsed":279231,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}},"outputId":"adbfa2d6-ba11-437d-d580-3239f7e05f63"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 3 candidates, totalling 15 fits\n","\n","train_score: 99.91%\n","valid_score: 88.26%\n","test_score: 88.42%\n"]}]}]}
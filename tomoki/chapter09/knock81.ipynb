{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1WQaKTKjLD18bN-GQAKiE-5PuVjbUl8C9","authorship_tag":"ABX9TyOQAwjp5fxojkRlQiGurWcV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bKR2pGeZoSM","executionInfo":{"status":"ok","timestamp":1719378511675,"user_tz":-540,"elapsed":6100,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}},"outputId":"6a0244a8-1be6-4a04-ea08-e5f5513e194e"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 1\n","to 2\n","in 3\n","on 4\n","UPDATE 5\n","The 6\n","as 7\n","for 8\n","To 9\n","of 10\n"]}],"source":["#No81(RNNによる予測)\n","#RNN:中間層で計算した情報を、再び中間層の入力として繰り返し処理を行うという特徴をもつ\n","import pandas as pd\n","import re\n","import numpy as np\n","\n","# 前処理があまいところがあったので、作成し直す(No50と同じ)\n","file = '/content/drive/MyDrive/newsCorpora.csv'\n","data = pd.read_csv(file, encoding='utf-8', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","data = data.replace('\"', \"'\")\n","# 特定のpublisherのみ抽出\n","publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n","data = data.loc[data['PUBLISHER'].isin(publishers), ['TITLE', 'CATEGORY']].reset_index(drop=True)\n","\n","# 前処理\n","def preprocessing(text):\n","    text_clean = re.sub(r'[\\\"\\'.,:;\\(\\)#\\|\\*\\+\\!\\?#$%&/\\]\\[\\{\\}]', '', text)\n","    text_clean = re.sub('[0-9]+', '0', text_clean)\n","    text_clean = re.sub('\\s-\\s', ' ', text_clean)\n","    return text_clean\n","\n","data['TITLE'] = data['TITLE'].apply(preprocessing)\n","\n","# 学習用、検証用、評価用に分割する\n","from sklearn.model_selection import train_test_split\n","\n","train, valid_test = train_test_split(data, test_size=0.2, shuffle=True, random_state=64, stratify=data['CATEGORY'])\n","valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=64, stratify=valid_test['CATEGORY'])\n","\n","train = train.reset_index(drop=True)\n","valid = valid.reset_index(drop=True)\n","test = test.reset_index(drop=True)\n","\n","# 単語の頻度\n","from collections import Counter\n","words = []\n","for text in train['TITLE']:\n","  #空文字で区切って、wordに追加していく\n","    for word in text.rstrip().split():\n","        words.append(word)\n","#数える\n","c = Counter(words)\n","#辞書の作成\n","word2id = {}\n","for i, cnt in enumerate(c.most_common()):\n","  ##出現頻度2回以上の単語のみ辞書に追加\n","    if cnt[1] > 1:\n","        word2id[cnt[0]] = i + 1\n","# 出現頻度上位10単語\n","for i, cnt in enumerate(word2id.items()):\n","    if i >= 10:\n","        break\n","    print(cnt[0], cnt[1])\n","# 単語のID化\n","def tokenizer(text):\n","    words = text.rstrip().split()\n","#単語辞書からそのwordのidを取得．ない場合は0を返す\n","    return [word2id.get(word, 0) for word in words]\n"]},{"cell_type":"code","source":["#modelの可視化\n","!pip install torchinfo\n","from torch import nn\n","import random\n","import torch\n","from torch import nn\n","import torch.utils.data as data\n","from torchinfo import summary\n","# 乱数のシードを設定\n","# parserなどで指定\n","seed = 1234\n","random.seed(seed)\n","np.random.seed(seed)\n","#再現性を保つ\n","torch.manual_seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","#seed値の作成\n","def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","g = torch.Generator()\n","g.manual_seed(seed)\n","#RNNクラスの作成\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, emb_size, padding_idx, hidden_size, output_size, num_layers=1):\n","        super().__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","        self.rnn = nn.RNN(emb_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","    #順伝播\n","    def forward(self, x, h0=None):\n","        x = self.emb(x)\n","        #予測ラベルxと次の隠れ状態\n","        x, h = self.rnn(x, h0)\n","        #最後の隠れ状態xの出力層\n","        x = x[:, -1, :]\n","        logits = self.fc(x)\n","        return logits\n","\n","# パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 2  # 辞書のID数 + unknown + パディングID(ID化する単語の種類)\n","EMB_SIZE = 300 #埋め込みベクトルのサイズ\n","PADDING_IDX = len(set(word2id.values())) + 1 #各データの単語数を揃えるために空きを埋めるindex\n","OUTPUT_SIZE = 4 #分類するクラス数\n","HIDDEN_SIZE = 50 #RNNの隠れ層のサイズ\n","NUM_LAYERS = 1 #RNNの層数\n","\n","# モデルの定義\n","model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_fBNW5kazJr","executionInfo":{"status":"ok","timestamp":1719378559612,"user_tz":-540,"elapsed":5306,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}},"outputId":"a654092a-39f3-4479-a0b4-6717eeff85ac"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n","RNN(\n","  (emb): Embedding(4598, 300, padding_idx=4597)\n","  (rnn): RNN(300, 50, batch_first=True)\n","  (fc): Linear(in_features=50, out_features=4, bias=True)\n",")\n"]}]}]}
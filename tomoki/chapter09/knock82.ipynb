{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"144f0lhjaq1IyJh_8rVn48dJT-9ADk8wT","authorship_tag":"ABX9TyM+NTGmZGwUpi+K8Pvv0x80"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["#No82(確率的勾配降下法による学習)\n","import pandas as pd\n","import re\n","import numpy as np\n","\n","# 前処理があまいところがあったので、作成し直す\n","file = '/content/drive/MyDrive/newsCorpora.csv'\n","data = pd.read_csv(file, encoding='utf-8', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","data = data.replace('\"', \"'\")\n","# 特定のpublisherのみ抽出\n","publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n","data = data.loc[data['PUBLISHER'].isin(publishers), ['TITLE', 'CATEGORY']].reset_index(drop=True)\n","\n","# 前処理\n","def preprocessing(text):\n","    text_clean = re.sub(r'[\\\"\\'.,:;\\(\\)#\\|\\*\\+\\!\\?#$%&/\\]\\[\\{\\}]', '', text)\n","    text_clean = re.sub('[0-9]+', '0', text_clean)\n","    text_clean = re.sub('\\s-\\s', ' ', text_clean)\n","    return text_clean\n","\n","data['TITLE'] = data['TITLE'].apply(preprocessing)\n","\n","# 学習用、検証用、評価用に分割する\n","from sklearn.model_selection import train_test_split\n","\n","train, valid_test = train_test_split(data, test_size=0.2, shuffle=True, random_state=64, stratify=data['CATEGORY'])\n","valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=64, stratify=valid_test['CATEGORY'])\n","\n","train = train.reset_index(drop=True)\n","valid = valid.reset_index(drop=True)\n","test = test.reset_index(drop=True)\n","\n","# 単語の頻度\n","from collections import Counter\n","words = []\n","for text in train['TITLE']:\n","  #空文字で区切って、wordに追加していく\n","    for word in text.rstrip().split():\n","        words.append(word)\n","#数える\n","c = Counter(words)\n","#辞書の作成\n","word2id = {}\n","for i, cnt in enumerate(c.most_common()):\n","  ##出現頻度2回以上の単語のみ辞書に追加\n","    if cnt[1] > 1:\n","        word2id[cnt[0]] = i + 1\n","# 出現頻度上位10単語\n","for i, cnt in enumerate(word2id.items()):\n","    if i >= 10:\n","        break\n","    print(cnt[0], cnt[1])\n","# 単語のID化\n","def tokenizer(text):\n","    words = text.rstrip().split()\n","#単語辞書からそのwordのidを取得．ない場合は0を返す\n","    return [word2id.get(word, 0) for word in words]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JqbqbWI7pNFf","executionInfo":{"status":"ok","timestamp":1719380518095,"user_tz":-540,"elapsed":4365,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}},"outputId":"79bb5163-bc1f-4d50-cb17-9282fc917d51"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["0 1\n","to 2\n","in 3\n","on 4\n","UPDATE 5\n","The 6\n","as 7\n","for 8\n","To 9\n","of 10\n"]}]},{"cell_type":"code","source":["#modelの可視化\n","!pip install torchinfo\n","from torch import nn\n","import random\n","import torch\n","from torch import nn\n","import torch.utils.data as data\n","from torchinfo import summary\n","# 乱数のシードを設定\n","# parserなどで指定\n","seed = 1234\n","random.seed(seed)\n","np.random.seed(seed)\n","#再現性を保つ\n","torch.manual_seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","#seed値の作成\n","def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","g = torch.Generator()\n","g.manual_seed(seed)\n","#RNNクラスの作成\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, emb_size, padding_idx, hidden_size, output_size, num_layers=1):\n","        super().__init__()\n","        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","        self.rnn = nn.RNN(emb_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, h0=None):\n","        x = self.emb(x)\n","        #予測ラベルxと次の隠れ状態\n","        x, h = self.rnn(x, h0)\n","        #最後の隠れ状態xの出力層\n","        x = x[:, -1, :]\n","        logits = self.fc(x)\n","        return logits\n","\n","# パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 2  # 辞書のID数 + unknown + パディングID(ID化する単語の種類)\n","EMB_SIZE = 300 #埋め込みベクトルのサイズ\n","PADDING_IDX = len(set(word2id.values())) + 1 #各データの単語数を揃えるために空きを埋めるindex\n","OUTPUT_SIZE = 4 #分類するクラス数\n","HIDDEN_SIZE = 50 #RNNの隠れ層のサイズ\n","NUM_LAYERS = 1 #RNNの層数\n","\n","# モデルの定義\n","model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4egh1bR-peRv","executionInfo":{"status":"ok","timestamp":1719380533818,"user_tz":-540,"elapsed":10730,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}},"outputId":"b8bdd48f-32df-44f9-9ff7-a2338aac119e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n","RNN(\n","  (emb): Embedding(4598, 300, padding_idx=4597)\n","  (rnn): RNN(300, 50, batch_first=True)\n","  (fc): Linear(in_features=50, out_features=4, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n","Y_train = torch.from_numpy(train['CATEGORY'].map(category_dict).values)\n","Y_valid = torch.from_numpy(valid['CATEGORY'].map(category_dict).values)\n","Y_test = torch.from_numpy(test['CATEGORY'].map(category_dict).values)\n","print(Y_train.size())\n","print(Y_train)\n","\n","class NewsDataset(data.Dataset):\n","    def __init__(self, X, y, phase='train'):\n","      #X: 単語ベクトルの平均をまとめたテンソル\n","      #y: カテゴリをラベル化したテンソル\n","        self.X = X['TITLE']\n","        self.y = y\n","        #学習か訓練かを設定する\n","        self.phase = phase\n","\n","    def __len__(self):\n","        #全データサイズを返す\n","        return len(self.y)\n","\n","    def __getitem__(self, idx):\n","        #idxに対応するテンソル形式のデータとラベルを取得\n","        inputs = torch.tensor(tokenizer(self.X[idx]))\n","        return inputs, self.y[idx]\n","\n","train_dataset = NewsDataset(train, Y_train, phase='train')\n","valid_dataset = NewsDataset(valid, Y_valid, phase='val')\n","test_dataset = NewsDataset(test, Y_test, phase='val')\n","idx = 0\n","\n","\n","batch_size = 1\n","#Dataloaderの作成\n","train_dataloader = data.DataLoader(\n","            train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n","valid_dataloader = data.DataLoader(\n","            valid_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","test_dataloader = data.DataLoader(\n","            test_dataset, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)\n","\n","dataloaders_dict = {'train': train_dataloader,\n","                    'val': valid_dataloader,\n","                    'test': test_dataloader,}\n","\n","batch_iter = iter(dataloaders_dict['train'])\n","inputs, labels = next(batch_iter)\n","\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","# 学習用の関数を定義\n","def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n","    train_loss = []\n","    train_acc = []\n","    valid_loss = []\n","    valid_acc = []\n","    # epochのループ\n","    for epoch in range(num_epochs):\n","        print('Epoch {} / {}'.format(epoch + 1, num_epochs))\n","        print('--------------------------------------------')\n","\n","        # epochごとの学習と検証のループ\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                net.train() # 訓練モード\n","            else:\n","                net.eval() # 検証モード\n","\n","            epoch_loss = 0.0 # epochの損失和\n","            epoch_corrects = 0 # epochの正解数\n","\n","            # データローダーからミニバッチを取り出すループ\n","            for inputs, labels in tqdm(dataloaders_dict[phase]):\n","                optimizer.zero_grad() # optimizerを初期化\n","\n","                # 順伝播計算(forward)\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = net(inputs)\n","                    loss = criterion(outputs, labels) # 損失を計算\n","                    _, preds = torch.max(outputs, 1) # ラベルを予想\n","\n","                    # 訓練時は逆伝播\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                    # イテレーション結果の計算\n","                    # lossの合計を更新\n","                    epoch_loss += loss.item() * inputs.size(0)\n","                    # 正解数の合計を更新\n","                    epoch_corrects += torch.sum(preds == labels.data)\n","\n","            # epochごとのlossと正解率の表示\n","            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n","            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n","            if phase == 'train':\n","                train_loss.append(epoch_loss)\n","                train_acc.append(epoch_acc)\n","            else:\n","                valid_loss.append(epoch_loss)\n","                valid_acc.append(epoch_acc)\n","\n","            print('{} Loss: {:.4f}, Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","    return train_loss, train_acc, valid_loss, valid_acc\n","\n","# 学習を実行する\n","\n","# モデルの定義\n","net = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)\n","net.train()\n","\n","# 損失関数の定義\n","criterion = nn.CrossEntropyLoss()\n","\n","# 最適化手法の定義\n","optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","num_epochs = 5\n","train_loss, train_acc, valid_loss, valid_acc = train_model(net,\n","            dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7K5CRRnkpr-a","executionInfo":{"status":"ok","timestamp":1719380772680,"user_tz":-540,"elapsed":233418,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}},"outputId":"bca254d0-bdcc-4bb8-dbc2-a44b59089695"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3709])\n","tensor([3, 1, 1,  ..., 2, 0, 0])\n","Epoch 1 / 5\n","--------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3709/3709 [00:35<00:00, 104.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 1.1440, Acc: 0.5071\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 464/464 [00:00<00:00, 1286.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 1.0463, Acc: 0.5841\n","Epoch 2 / 5\n","--------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3709/3709 [00:47<00:00, 77.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.8549, Acc: 0.6854\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 464/464 [00:00<00:00, 1330.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 0.9782, Acc: 0.6164\n","Epoch 3 / 5\n","--------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3709/3709 [00:48<00:00, 76.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.6700, Acc: 0.7644\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 464/464 [00:00<00:00, 1408.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 1.2615, Acc: 0.5560\n","Epoch 4 / 5\n","--------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3709/3709 [00:48<00:00, 76.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.5785, Acc: 0.7902\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 464/464 [00:00<00:00, 1320.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["val Loss: 0.9237, Acc: 0.7198\n","Epoch 5 / 5\n","--------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3709/3709 [00:49<00:00, 75.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.4891, Acc: 0.8266\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 464/464 [00:00<00:00, 1347.47it/s]"]},{"output_type":"stream","name":"stdout","text":["val Loss: 0.9810, Acc: 0.6897\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1qLPVOSGOpazKnXrLCqH96sGvLzNr4UYe","timestamp":1719640264165},{"file_id":"192sktMro1RGKtObqOvYXMYQnu6X5Ih9s","timestamp":1719551739082}],"gpuType":"T4","mount_file_id":"1E_bvLralF0TlCRMuS6mki25i2zqVMss-","authorship_tag":"ABX9TyPT5qCara+2HBD+8nfwi+GU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAjsESdJHVwF","executionInfo":{"status":"ok","timestamp":1719641475403,"user_tz":-540,"elapsed":762,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}},"outputId":"7f78012b-b5a0-42a1-b835-6bd3ecc188e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 1\n","to 2\n","in 3\n","on 4\n","UPDATE 5\n","The 6\n","as 7\n","for 8\n","To 9\n","of 10\n"]}],"source":["#No86(畳み込みニューラルネットワーク (CNN))\n","#CNNは基本的に画像認識で使用されることが多い。自然言語処理においては、文をマトリックスに見立てる\n","import pandas as pd\n","import re\n","import numpy as np\n","\n","# 前処理があまいところがあったので、作成し直す\n","file = '/content/drive/MyDrive/newsCorpora.csv'\n","data = pd.read_csv(file, encoding='utf-8', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","data = data.replace('\"', \"'\")\n","# 特定のpublisherのみ抽出\n","publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n","data = data.loc[data['PUBLISHER'].isin(publishers), ['TITLE', 'CATEGORY']].reset_index(drop=True)\n","\n","# 前処理\n","def preprocessing(text):\n","    text_clean = re.sub(r'[\\\"\\'.,:;\\(\\)#\\|\\*\\+\\!\\?#$%&/\\]\\[\\{\\}]', '', text)\n","    text_clean = re.sub('[0-9]+', '0', text_clean)\n","    text_clean = re.sub('\\s-\\s', ' ', text_clean)\n","    return text_clean\n","\n","data['TITLE'] = data['TITLE'].apply(preprocessing)\n","\n","# 学習用、検証用、評価用に分割する\n","from sklearn.model_selection import train_test_split\n","\n","train, valid_test = train_test_split(data, test_size=0.2, shuffle=True, random_state=64, stratify=data['CATEGORY'])\n","valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=64, stratify=valid_test['CATEGORY'])\n","\n","train = train.reset_index(drop=True)\n","valid = valid.reset_index(drop=True)\n","test = test.reset_index(drop=True)\n","\n","# 単語の頻度\n","from collections import Counter\n","words = []\n","for text in train['TITLE']:\n","  #空文字で区切って、wordに追加していく\n","    for word in text.rstrip().split():\n","        words.append(word)\n","#数える\n","c = Counter(words)\n","#辞書の作成\n","word2id = {}\n","for i, cnt in enumerate(c.most_common()):\n","  ##出現頻度2回以上の単語のみ辞書に追加\n","    if cnt[1] > 1:\n","        word2id[cnt[0]] = i + 1\n","# 出現頻度上位10単語\n","for i, cnt in enumerate(word2id.items()):\n","    if i >= 10:\n","        break\n","    print(cnt[0], cnt[1])\n","# 単語のID化\n","def tokenizer(text):\n","    words = text.rstrip().split()\n","#単語辞書からそのwordのidを取得．ない場合は0を返す\n","    return [word2id.get(word, 0) for word in words]\n","sample = train.at[0, 'TITLE']"]},{"cell_type":"code","source":["#modelの可視化\n","!pip install torchinfo\n","from torch import nn\n","import random\n","import torch\n","from torch import nn\n","import torch.utils.data as data\n","from torchinfo import summary\n","\n","# 乱数のシードを設定\n","# parserなどで指定\n","seed = 1234\n","random.seed(seed)\n","np.random.seed(seed)\n","#再現性を保つ\n","torch.manual_seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","#seed値の作成\n","def seed_worker(worker_id):\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)\n","\n","g = torch.Generator()\n","g.manual_seed(seed)\n","\n","from gensim.models import KeyedVectors\n","\n","# 事前学習済みの単語ベクトル\n","file = '/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz'\n","model = KeyedVectors.load_word2vec_format(file, binary=True)\n","\n","# 学習済み単語ベクトルの取得\n","VOCAB_SIZE = len(set(word2id.values())) + 2 #辞書のID数 + unknown + パディングID(ID化する単語の種類)\n","EMB_SIZE = 300 #埋め込みベクトルのサイズ\n","weights = np.zeros((VOCAB_SIZE, EMB_SIZE)) #学習済み単語ベクトルの初期化\n","words_in_pretrained = 0\n","#学習済み単語ベクトルの取得(単語が無いときは正規乱数で初期化)\n","for i, word in enumerate(word2id.keys()):\n","    try:\n","        weights[i] = model[word]\n","        words_in_pretrained += 1\n","    except KeyError:\n","        weights[i] = np.random.normal(scale=0.1, size=(EMB_SIZE,))\n","weights = torch.from_numpy(weights.astype((np.float32)))\n","\n","print(f'学習済みベクトル利用単語数: {words_in_pretrained} / {VOCAB_SIZE}')\n","print(weights.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hrN52tskHn02","executionInfo":{"status":"ok","timestamp":1719641298809,"user_tz":-540,"elapsed":77271,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}},"outputId":"b9c5d09d-82ea-4e1a-d027-76274331624e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n","学習済みベクトル利用単語数: 4356 / 4598\n","torch.Size([4598, 300])\n"]}]},{"cell_type":"code","source":["category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n","Y_train = torch.from_numpy(train['CATEGORY'].map(category_dict).values)\n","Y_valid = torch.from_numpy(valid['CATEGORY'].map(category_dict).values)\n","Y_test = torch.from_numpy(test['CATEGORY'].map(category_dict).values)\n","print(Y_train.size())\n","print(Y_train)\n","\n","class NewsDataset(data.Dataset):\n","    def __init__(self, X, y, phase='train'):\n","      #X: 単語ベクトルの平均をまとめたテンソル\n","      #y: カテゴリをラベル化したテンソル\n","        self.X = X['TITLE']\n","        self.y = y\n","        #学習か訓練かを設定する\n","        self.phase = phase\n","\n","    def __len__(self):\n","        #全データサイズを返す\n","        return len(self.y)\n","\n","    def __getitem__(self, idx):\n","        #idxに対応するテンソル形式のデータとラベルを取得\n","        inputs = torch.tensor(tokenizer(self.X[idx]))\n","        return inputs, self.y[idx]\n","\n","train_dataset = NewsDataset(train, Y_train, phase='train')\n","valid_dataset = NewsDataset(valid, Y_valid, phase='val')\n","test_dataset = NewsDataset(test, Y_test, phase='val')\n","idx = 0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOygedXRJjpS","executionInfo":{"status":"ok","timestamp":1719641302554,"user_tz":-540,"elapsed":323,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}},"outputId":"9fb4c613-74d4-43e3-e335-dd4b7eadacb2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3709])\n","tensor([3, 1, 1,  ..., 2, 0, 0])\n"]}]},{"cell_type":"code","source":["#自動的にバッチ内の文章の最大単語数でそろえるようにする。\n","def collate_fn(batch):\n","    sequences = [x[0] for x in batch]\n","    labels = torch.LongTensor([x[1] for x in batch])\n","    x = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=PADDING_IDX)\n","    return x, labels\n","\n","# DataLoaderを作成\n","batch_size = 64\n","\n","train_dataloader = data.DataLoader(\n","            train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, worker_init_fn=seed_worker, generator=g)\n","valid_dataloader = data.DataLoader(\n","            valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, worker_init_fn=seed_worker, generator=g)\n","test_dataloader = data.DataLoader(\n","            test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, worker_init_fn=seed_worker, generator=g)\n","\n","dataloaders_dict = {'train': train_dataloader,\n","                    'val': valid_dataloader,\n","                    'test': test_dataloader,}\n"],"metadata":{"id":"DsrEUo57JwcV","executionInfo":{"status":"ok","timestamp":1719641305461,"user_tz":-540,"elapsed":4,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from torch.nn import functional as F\n","#CNNクラスの作成\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, kernel_heights, stride, padding, emb_weights=None):\n","        super().__init__()\n","        if emb_weights != None:  # 指定があれば埋め込み層の重みをemb_weightsで初期化\n","            self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n","        else:\n","            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","        self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding, 0))\n","        self.drop = nn.Dropout(0.4)\n","        self.fc = nn.Linear(out_channels, output_size)\n","\n","    def forward(self, x):\n","        #テンソルの1階に要素数１の次元を追加する\n","        emb = self.emb(x).unsqueeze(1)\n","        #convolution(畳み込み)\n","        #第1引数はその入力のチャネル数,第2引数は畳み込み後のチャネル数,第3引数は畳み込みをするための正方形フィルタ(カーネル)の1辺のサイズ\n","        conv = self.conv(emb)\n","        #要素を削除して、活性化関数(relu)に通す\n","        act = F.relu(conv.squeeze(3))\n","        #特徴量をサンプリングして減らす処理\n","        max_pool = F.max_pool1d(act, act.size()[2])\n","        #全結合層\n","        logits = self.fc(self.drop(max_pool.squeeze(2)))\n","        return logits\n","\n","# パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 2\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values())) + 1 ##各データの単語数を揃えるために空きを埋めるindex\n","OUTPUT_SIZE = 4\n","#出力のチャネル数\n","OUT_CHANNELS = 100\n","#カーネルのサイズ\n","KERNEL_HEIGHTS = 3\n","#フィルタが移動する幅(基本は１)\n","STRIDE = 1\n","#埋め込みあり\n","PADDING = 1\n","\n","# モデルの定義\n","model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=weights)\n","x = torch.tensor([tokenizer(sample)], dtype=torch.int64)\n","print(x)\n","print(x.size())\n","print(nn.Softmax(dim=-1)(model(x)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-Vmo8bYmUtr","executionInfo":{"status":"ok","timestamp":1719641480278,"user_tz":-540,"elapsed":358,"user":{"displayName":"Tomoki Kera","userId":"06617195934752380956"}},"outputId":"288d2cd0-0f0b-44f1-e74d-ce31b4cc4cd5"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1546, 2903,    0, 2904,  346, 1227,    0,  424,  314,  716]])\n","torch.Size([1, 10])\n","tensor([[0.2340, 0.2397, 0.2903, 0.2360]], grad_fn=<SoftmaxBackward0>)\n"]}]}]}
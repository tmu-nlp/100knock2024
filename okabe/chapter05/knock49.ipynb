{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 49. 名詞間の係り受けパスの抽出Permalink\n",
    "## 文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "##  - 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "##  - 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "## また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "## 文節iから構文木の根に至る経路上に文節jが存在する場合: \n",
    "##    文節iから文節jのパスを表示\n",
    "\n",
    "## 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: \n",
    "##    文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示\n",
    "\n",
    "## 「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． \n",
    "## CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
    "\n",
    "## Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
    "## Xは | Yの -> 会議で | 作り出した\n",
    "## Xは | Yで | 作り出した\n",
    "## Xは | Yという -> 用語を | 作り出した\n",
    "## Xは | Yを | 作り出した\n",
    "## Xに関する -> Yの\n",
    "## Xに関する -> 最初の -> Yで\n",
    "## Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
    "## Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
    "## Xの -> Yで\n",
    "## Xの -> 会議で | Yという -> 用語を | 作り出した\n",
    "## Xの -> 会議で | Yを | 作り出した\n",
    "## Xで | Yという -> 用語を | 作り出した\n",
    "## Xで | Yを | 作り出した\n",
    "## Xという -> Yを\n",
    "\n",
    "from knock41 import sentences\n",
    "from itertools import combinations\n",
    "import re\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    nouns = []  # 名詞を含むチャンクのインデックスを格納するリスト\n",
    "    for i, chunk in enumerate(sentence.chunks):\n",
    "        if '名詞' in [morph.pos for morph in chunk.morphs]:  # chunkに名詞の形態素が入っているか\n",
    "            nouns.append(i)  # 名詞が含まれたときにチャンクのインデックスをnounsに追加\n",
    "    for i, j in combinations(nouns, 2):  # 名詞を含む文節のペア\n",
    "        path_i = []\n",
    "        path_j = []\n",
    "        while i != j:\n",
    "            if i < j:\n",
    "                path_i.append(i)\n",
    "                i = sentence.chunks[i].dst\n",
    "            else:\n",
    "                path_j.append(j)\n",
    "                j = sentence.chunks[j].dst\n",
    "\n",
    "        # 1つ目のケース．チャンクiから根までのパスにチャンクjがあるなら成り立つはず．\n",
    "        if len(path_j) == 0:\n",
    "            chunk_X = ''\n",
    "            chunk_Y = ''\n",
    "            for morph in sentence.chunks[path_i[0]].morphs:\n",
    "                if morph.pos == '名詞':\n",
    "                    chunk_X += 'X'\n",
    "                elif morph.pos != '記号':\n",
    "                    chunk_X += morph.surface\n",
    "\n",
    "            for morph in sentence.chunks[i].morphs:\n",
    "                if morph.pos == '名詞':\n",
    "                    chunk_Y += 'Y'\n",
    "                elif morph.pos != '記号':\n",
    "                    chunk_Y += morph.surface\n",
    "\n",
    "            chunk_X = re.sub('X+', 'X', chunk_X)  # 連結名詞もXで表現する\n",
    "            chunk_Y = re.sub('Y+', 'Y', chunk_Y)  # Yも同様\n",
    "\n",
    "            mid_path = []\n",
    "            for n in path_i[1:]:\n",
    "                for morph in sentence.chunks[n].morphs:\n",
    "                    if morph.pos != '記号':\n",
    "                        sf += morph.surface\n",
    "                mid_path.append(sf)\n",
    "                sf = ''\n",
    "\n",
    "            path_XtoY = [chunk_X] + mid_path + [chunk_Y]\n",
    "            print(' -> '.join(path_XtoY))\n",
    "\n",
    "        # 2つ目のケース\n",
    "        else:\n",
    "            chunk_X = ''\n",
    "            chunk_Y = ''\n",
    "            for morph in sentence.chunks[path_i[0]].morphs:\n",
    "                if morph.pos == '名詞':\n",
    "                    chunk_X += 'X'\n",
    "                elif morph.pos != '記号':\n",
    "                    chunk_X += morph.surface\n",
    "\n",
    "            for morph in sentence.chunks[path_j[0]].morphs:\n",
    "                if morph.pos == '名詞':\n",
    "                    chunk_Y += 'Y'\n",
    "                elif morph.pos != '記号':\n",
    "                    chunk_Y += morph.surface\n",
    "\n",
    "            chunk_k = ''.join(\n",
    "                [morph.surface if morph.pos != '記号' else '' for morph in sentence.chunks[i].morphs])  # 二つの名詞からdstを辿って重なったchunk\n",
    "            chunk_X = re.sub('X+', 'X', chunk_X)\n",
    "            chunk_Y = re.sub('Y+', 'Y', chunk_Y)\n",
    "\n",
    "            mid_path_X = []\n",
    "            mid_path_Y = []\n",
    "            sf = ''\n",
    "            for n in path_i[1:]:\n",
    "                for morph in sentence.chunks[n].morphs:\n",
    "                    if morph.pos != '記号':\n",
    "                        sf += morph.surface\n",
    "                mid_path_X.append(sf)\n",
    "                sf = ''\n",
    "            for n in path_j[1:]:\n",
    "                for morph in sentence.chunks[n].morphs:\n",
    "                    if morph.pos != '記号':\n",
    "                        sf += morph.surface\n",
    "                mid_path_Y.append(sf)\n",
    "                sf = ''\n",
    "\n",
    "            path_X = [chunk_X] + mid_path_X\n",
    "            path_Y = [chunk_Y] + mid_path_Y\n",
    "\n",
    "            print(' | '.join(\n",
    "                [' -> '.join(path_X), ' -> '.join(path_Y), chunk_k]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

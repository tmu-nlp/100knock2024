{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMP9UBLRMTbLF+iWU8LJrZ0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# knock86"],"metadata":{"id":"LfLPqkYdKXRz"}},{"cell_type":"code","source":["m = nn.Dropout(0.3)\n","input = torch.randn(2, 10)\n","output = m(input)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r0f9-O9BKa_1","executionInfo":{"status":"ok","timestamp":1721630614449,"user_tz":-540,"elapsed":18,"user":{"displayName":"OSADA MANATO","userId":"03457769063882444574"}},"outputId":"cda810f9-113d-4839-c5a7-caa7e488eb91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.0000,  0.0271, -0.7837, -0.7760, -0.4227, -0.7098, -0.0000, -0.0000,\n","         -0.0000, -0.5212],\n","        [-0.0000, -0.0506,  0.0330, -0.6011,  1.3485, -0.1309,  0.5920, -0.9393,\n","         -0.0000, -2.4764]])\n"]}]},{"cell_type":"code","source":["from torch.nn import functional as F\n","\n","class CNN(nn.Module):\n","  def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, kernel_heights, stride, padding, emb_weights=None):\n","    super().__init__()\n","    if emb_weights != None:  # 指定があれば埋め込み層の重みをemb_weightsで初期化\n","      self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n","    else:\n","      self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","    self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding, 0))\n","    self.drop = nn.Dropout(0.3)\n","    self.fc = nn.Linear(out_channels, output_size)\n","\n","  def forward(self, x):\n","    # x.size() = (batch_size, seq_len)\n","    emb = self.emb(x).unsqueeze(1)\n","    # emb.size() = (batch_size, 1, seq_len, emb_size)\n","    conv = self.conv(emb)\n","    # conv.size() = (batch_size, out_channels, seq_len, 1)\n","    act = F.relu(conv.squeeze(3))\n","    # act.size() = (batch_size, out_channels, seq_len)\n","    max_pool = F.max_pool1d(act, act.size()[2])\n","    # max_pool.size() = (batch_size, out_channels, 1) -> seq_len方向に最大値を取得\n","    out = self.fc(self.drop(max_pool.squeeze(2)))\n","    # out.size() = (batch_size, output_size)\n","    return out"],"metadata":{"id":"Ns5xBpNjKfDu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 1\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values()))\n","OUTPUT_SIZE = 4\n","OUT_CHANNELS = 100\n","KERNEL_HEIGHTS = 3\n","STRIDE = 1\n","PADDING = 1\n","\n","# モデルの定義\n","model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=weights)\n","\n","# 先頭10件の予測値取得\n","for i in range(10):\n","  X = dataset_train[i]['inputs']\n","  print(torch.softmax(model(X.unsqueeze(0)), dim=-1)) #unsqueezeでbatch_sizeを1として追加"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NGNscjnKuqw","executionInfo":{"status":"ok","timestamp":1721630614449,"user_tz":-540,"elapsed":13,"user":{"displayName":"OSADA MANATO","userId":"03457769063882444574"}},"outputId":"a6f4fd27-1d8c-4960-e011-58da0fad1d56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2810, 0.2778, 0.2211, 0.2201]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2493, 0.2811, 0.2615, 0.2082]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2804, 0.3009, 0.2599, 0.1588]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2868, 0.2740, 0.2242, 0.2151]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2617, 0.2865, 0.2542, 0.1975]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2226, 0.3255, 0.2520, 0.1999]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2715, 0.2709, 0.2308, 0.2269]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2838, 0.2763, 0.2565, 0.1833]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2529, 0.2881, 0.2480, 0.2111]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2751, 0.2601, 0.2910, 0.1738]], grad_fn=<SoftmaxBackward0>)\n"]}]}]}
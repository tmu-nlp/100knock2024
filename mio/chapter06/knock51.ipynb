{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SngVUCbTFghN",
        "outputId": "771dda3e-e661-4ade-da7f-91fd027b8b2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    aa  aaliyah  abbvie  abc     about  about the  about to  above  abramson  \\\n",
            "0  0.0      0.0     0.0  0.0  0.000000        0.0       0.0    0.0       0.0   \n",
            "1  0.0      0.0     0.0  0.0  0.274054        0.0       0.0    0.0       0.0   \n",
            "2  0.0      0.0     0.0  0.0  0.000000        0.0       0.0    0.0       0.0   \n",
            "3  0.0      0.0     0.0  0.0  0.000000        0.0       0.0    0.0       0.0   \n",
            "4  0.0      0.0     0.0  0.0  0.000000        0.0       0.0    0.0       0.0   \n",
            "\n",
            "   abuse  ...  young  your  your mother   yr  yr high  yuan  zac  zac efron  \\\n",
            "0    0.0  ...    0.0   0.0          0.0  0.0      0.0   0.0  0.0        0.0   \n",
            "1    0.0  ...    0.0   0.0          0.0  0.0      0.0   0.0  0.0        0.0   \n",
            "2    0.0  ...    0.0   0.0          0.0  0.0      0.0   0.0  0.0        0.0   \n",
            "3    0.0  ...    0.0   0.0          0.0  0.0      0.0   0.0  0.0        0.0   \n",
            "4    0.0  ...    0.0   0.0          0.0  0.0      0.0   0.0  0.0        0.0   \n",
            "\n",
            "   zendaya  zone  \n",
            "0      0.0   0.0  \n",
            "1      0.0   0.0  \n",
            "2      0.0   0.0  \n",
            "3      0.0   0.0  \n",
            "4      0.0   0.0  \n",
            "\n",
            "[5 rows x 2737 columns]\n"
          ]
        }
      ],
      "source": [
        "#51. 特徴量抽出\n",
        "#学習データ，検証データ，評価データから特徴量を抽出し，\n",
        "#それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ．\n",
        "#なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．\n",
        "#記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．\n",
        "\n",
        "#方針１　ライブラリのインポート  Pandas：データフレームの操作、\n",
        "#　　　　                        string、re：テキストデータの前処理、\n",
        "#　　　　　　　　　　　　　　　　sklearn.feature_extractionモジュール：テキストや画像などの形式からなるデータセットから特徴抽出\n",
        "#                                   TfidfVectorizer：TF-IDF(索引語頻度逆文書頻度)により特徴量\n",
        "#　　　　　　　　　　　　　　　　　　　　　　　　　　(TF-IDF)=(TF（文書出現頻度）)* (IDF（逆文書頻度）)　　※逆文書頻度：単語のレア度\n",
        "#　　　　　　　　　　　　　　　　　　　　　　　　　　IDF= log(総文書数/指定単語を含む文書数)\n",
        "#                                   ※CountVectorizer：出現単語を数えて特徴量とする\n",
        "#                                                    　問題点：出現数の多い単語のベクトルが極端に強くなる\n",
        "\n",
        "#方針２　前処理を関数化する　prep()関数：テキストデータの句読点の削除、小文字化、数字の削除\n",
        "\n",
        "#方針３　train.txt、valid.txt、test.txtからread_csv()でデータを読み込む\n",
        "\n",
        "#方針４　データの結合と前処理: 3つのデータセットをconcat()で結合し、reset_index()結合後のDataFrameのインデックスを連続的に整理\n",
        "#                              →TITLEカラムのデータにapplyで前処理関数を適用\n",
        "\n",
        "#方針５　データを「トレーニング、検証データセット/テストデータセット」分割し、TfidfVectorizerでtf-idf特徴量の抽出\n",
        "\n",
        "#方針６　統計量を計算し、その統計量をもとに特徴ベクトルをデータフレーム化→train.feature.txt、valid.feature.txt、test.feature.txtとして保存\n",
        "\n",
        "#方針７　特徴ベクトル（トレーニングデータの最初の数行）を表示\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#作業１ライブラリのインポート\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer #BoW\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "\n",
        "#作業２：前処理の関数化\n",
        "def prep(text):\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text) #string.punctuation ：!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\n",
        "    text = text.lower()  # 小文字化\n",
        "    text = re.sub(\"[0-9]+\", \"\", text)  # 1つ以上の数字を\"\"で置換\n",
        "    return text\n",
        "\n",
        "#作業３：テキストファイル読み込み\n",
        "header_name = ['TITLE', 'CATEGORY']\n",
        "\n",
        "train = pd.read_csv('./train.txt', header=None, sep='\\t', names=header_name)\n",
        "valid = pd.read_csv('./valid.txt', header=None, sep='\\t', names=header_name)\n",
        "test = pd.read_csv('./test.txt', header=None, sep='\\t', names=header_name)\n",
        "\n",
        "\n",
        "#作業４：データの結合,前処理関数適用\n",
        "df = pd.concat([train, valid, test], axis=0)    #concat([連結するDataFrame、Series(表1列) #カンマ区切り], axis=0 #縦方向)\n",
        "df.reset_index(drop=True, inplace=True) #drop=True：元のインデックス列を削除(DataFrameのサイズ削減)\n",
        "                                        #inplace=True:元のDataFrameを直接変更(デフォルトだと元のDataFrameを変更し、新しいDataFrameを返す）\n",
        "\n",
        "\n",
        "df[\"TITLE\"] = df[\"TITLE\"].apply(prep)\n",
        "\n",
        "#作業５：データ分割、特徴量抽出\n",
        "train_valid_data = df[:len(train)+len(valid)] #トレーニングデータと検証データの合計行数までの行をtrain_valid_dに割り当て\n",
        "test_data = df[len(train)+len(valid):]\n",
        "\n",
        "vec_tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2)) #min_df：dfの最小値を設定\n",
        "                                                           #n_gram_range：使用するn_gramを1~2gramに設定\n",
        "\n",
        "#作業６：統計量の正規化、データフレームに変換、それぞれファイルに保存\n",
        "\n",
        "#fit_transform()：fitで統計量を計算し、その統計量に基いてtransformで()の中身を正規化\n",
        "#TAの方のアドバイス：valid dataも省いて、トレーニングデータだけを学習する(fit_transform)べき！\n",
        "train_valid_f = vec_tfidf.fit_transform(train_valid_data[\"TITLE\"])\n",
        "test_f = vec_tfidf.transform(test_data[\"TITLE\"]) #1行前で計算した統計量をもとにtransform(テストデータを学習に用いてはいけない)\n",
        "\n",
        "\n",
        "#toarray()メソッド：疎行列(Scipyのcsr_matrix形式)→密行列(Numpyのndarray形式)に変換\n",
        "#                   (疎行列はメモリ効率が良いが、機械学習アルゴリズムやデータ解析の多くは密行列が前提)\n",
        "#                     ※CSR形式について：https://qiita.com/iwasaki620/items/603220d9102e82d4438e\n",
        "\n",
        "#get_feature_names_out()メソッド：ベクトライザが生成した特徴(単語、n-gram)のリストを返す\n",
        "#                                 データフレームの各列がどの単語、n-gramを表すかを示すラベル付与\n",
        "train_valid_vec = pd.DataFrame(train_valid_f.toarray(),\n",
        "                               columns=vec_tfidf.get_feature_names_out()) #列：特徴量、行：タイトル\n",
        "\n",
        "test_vec = pd.DataFrame(test_f.toarray(),\n",
        "                        columns=vec_tfidf.get_feature_names_out())\n",
        "\n",
        "train_vec = train_valid_vec[:len(train)]\n",
        "valid_vec = train_valid_vec[len(train):]\n",
        "\n",
        "train_vec.to_csv(\"./train.feature.txt\", sep=\"\\t\", index=False)\n",
        "valid_vec.to_csv(\"./valid.feature.txt\", sep=\"\\t\", index=False)\n",
        "test_vec.to_csv(\"./test.feature.txt\", sep=\"\\t\", index=False)\n",
        "\n",
        "#作業７：特徴ベクトルの表示\n",
        "print(train_vec.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#81. RNNによる予測\n",
        "\"\"\"\n",
        "ID番号で表現された単語列x=(x1,x2,…,xT)\n",
        "がある．ただし，T\n",
        "は単語列の長さ，xt∈RV\n",
        "は単語のID番号のone-hot表記である（V\n",
        "は単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列x\n",
        "からカテゴリy\n",
        "を予測するモデルとして，次式を実装せよ．\n",
        "\n",
        "h→0=0,h→t=RNN−→−−(emb(xt),h→t−1),y=softmax(W(yh)h→T+b(y))\n",
        "ただし，emb(x)∈Rdw\n",
        "は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），h→t∈Rdh\n",
        "は時刻t\n",
        "の隠れ状態ベクトル，RNN−→−−(x,h)\n",
        "は入力x\n",
        "と前時刻の隠れ状態h\n",
        "から次状態を計算するRNNユニット，W(yh)∈RL×dh\n",
        "は隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈RL\n",
        "はバイアス項である（dw,dh,L\n",
        "はそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニットRNN−→−−(x,h)\n",
        "には様々な構成が考えられるが，典型例として次式が挙げられる．\n",
        "\n",
        "RNN−→−−(x,h)=g(W(hx)x+W(hh)h+b(h))\n",
        "ただし，W(hx)∈Rdh×dw，W(hh)∈Rdh×dh,b(h)∈Rdh\n",
        "はRNNユニットのパラメータ，g\n",
        "は活性化関数（例えばtanh\n",
        "やReLUなど）である．\n",
        "\n",
        "なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータでy\n",
        "を計算するだけでよい．次元数などのハイパーパラメータは，dw=300,dh=50\n",
        "など，適当な値に設定せよ（以降の問題でも同様である）．\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "#from knock80 import *\n",
        "\n",
        "# RNNモデルの定義\n",
        "#NNクラス：PyTorchのnn.Moduleを継承\n",
        "#          __init__(初期化メソッド)で隠れ層のサイズ、語彙サイズ、埋め込み層のサイズ、パディングインデックス、および出力サイズを受け取\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, emb_size, pad_idx, output_size):\n",
        "        super().__init__()\n",
        "        self.hid_size = hidden_size\n",
        "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
        "        self.rnn = nn.RNN(emb_size, hidden_size,\n",
        "                          nonlinearity=\"tanh\", batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.batch_size = x.size()[0]\n",
        "        hidden = torch.zeros(1, self.batch_size, self.hid_size)\n",
        "        emb = self.emb(x)\n",
        "        out, hidden = self.rnn(emb, hidden)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# データセットの定義\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, x, y, tokenizer):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.x[idx]\n",
        "        inputs = self.tokenizer(text)\n",
        "\n",
        "        return {\n",
        "            'inputs': torch.tensor(inputs, dtype=torch.int64),\n",
        "            'labels': torch.tensor(self.y[idx], dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "# データの読み込み\n",
        "train = pd.read_csv(\"train.txt\", sep='\\t')\n",
        "valid = pd.read_csv(\"valid.txt\", sep='\\t')\n",
        "test = pd.read_csv(\"test.txt\", sep='\\t')\n",
        "\n",
        "# カテゴリを数値に変換\n",
        "category = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
        "y_train = torch.tensor(train['CATEGORY'].map(lambda x: category[x]).values)\n",
        "y_valid = torch.tensor(valid['CATEGORY'].map(lambda x: category[x]).values)\n",
        "y_test = torch.tensor(test['CATEGORY'].map(lambda x: category[x]).values)\n",
        "\n",
        "# データセットの作成\n",
        "dataset_train = NewsDataset(train[\"TITLE\"], y_train, w2id.return_id)\n",
        "dataset_valid = NewsDataset(valid[\"TITLE\"], y_valid, w2id.return_id)\n",
        "dataset_test = NewsDataset(test[\"TITLE\"], y_test, w2id.return_id)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # データセットのサイズを表示\n",
        "    print(f'len(Dataset): {len(dataset_train)}')\n",
        "    # データセットの一部を表示\n",
        "    print('Dataset[index]:')\n",
        "    for var in dataset_train[1]:\n",
        "        print(f'  {var}: {dataset_train[1][var]}')\n",
        "\n",
        "    # ハイパーパラメータの設定\n",
        "    VOCAB_SIZE = len(set(w2id.id_dict.values())) + 1\n",
        "    EMB_SIZE = 300\n",
        "    PADDING_IDX = len(set(w2id.id_dict.values()))\n",
        "    OUTPUT_SIZE = 4\n",
        "    HIDDEN_SIZE = 50\n",
        "\n",
        "    # モデルのインスタンス化\n",
        "    model = RNN(HIDDEN_SIZE, VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE)\n",
        "    # 予測結果を表示\n",
        "    for i in range(10):\n",
        "        X = dataset_train[i]['inputs']\n",
        "        print(torch.softmax(model(X.unsqueeze(0)), dim=-1))\n",
        "#6. データの読み込みと前処理：読み込んだデータをカテゴリを数値に変換\n",
        "#                             →NewsDatasetクラスを用いてデータセット作成\n",
        "train = pd.read_csv(\"train.txt\", sep='\\t')\n",
        "valid = pd.read_csv(\"valid.txt\", sep='\\t')\n",
        "test = pd.read_csv(\"test.txt\", sep='\\t')\n",
        "\n",
        "category = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
        "y_train = torch.tensor(train['CATEGORY'].map(lambda x: category[x]).values)\n",
        "y_valid = torch.tensor(valid['CATEGORY'].map(lambda x: category[x]).values)\n",
        "y_test = torch.tensor(test['CATEGORY'].map(lambda x: category[x]).values)\n",
        "\n",
        "dataset_train = NewsDataset(train[\"TITLE\"], y_train, w2id.return_id)\n",
        "dataset_valid = NewsDataset(valid[\"TITLE\"], y_valid, w2id.return_id)\n",
        "dataset_test = NewsDataset(test[\"TITLE\"], y_test, w2id.return_id)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f'len(Dataset): {len(dataset_train)}')\n",
        "    print('Dataset[index]:')\n",
        "    for var in dataset_train[1]:\n",
        "        print(f'  {var}: {dataset_train[1][var]}')\n",
        "\n",
        "    VOCAB_SIZE = len(set(w2id.id_dict.values())) + 1\n",
        "    EMB_SIZE = 300\n",
        "    PADDING_IDX = len(set(w2id.id_dict.values()))\n",
        "    OUTPUT_SIZE = 4\n",
        "    HIDDEN_SIZE = 50\n",
        "\n",
        "    model = RNN(HIDDEN_SIZE, VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE)\n",
        "    for i in range(10):\n",
        "        X = dataset_train[i]['inputs']\n",
        "        print(torch.softmax(model(X.unsqueeze(0)), dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTXUzIqhkb42",
        "outputId": "eeb5fec9-52af-44ed-8184-f63e62923599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(Dataset): 10672\n",
            "Dataset[index]:\n",
            "  inputs: tensor([2630, 1875,    0, 2630, 1875,   12, 2631,    0,    0,  212,   55,   25,\n",
            "         657])\n",
            "  labels: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a9ea6c48951d>:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'labels': torch.tensor(self.y[idx], dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1764, 0.1746, 0.4037, 0.2453]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2021, 0.2516, 0.3452, 0.2010]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.1890, 0.1602, 0.3613, 0.2896]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2091, 0.3281, 0.2623, 0.2005]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2502, 0.1494, 0.2585, 0.3420]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.1823, 0.2169, 0.2799, 0.3208]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2161, 0.3351, 0.2552, 0.1936]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.1310, 0.1538, 0.3206, 0.3946]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.1748, 0.3143, 0.2383, 0.2727]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.1486, 0.3140, 0.3293, 0.2081]], grad_fn=<SoftmaxBackward0>)\n",
            "len(Dataset): 10672\n",
            "Dataset[index]:\n",
            "  inputs: tensor([2630, 1875,    0, 2630, 1875,   12, 2631,    0,    0,  212,   55,   25,\n",
            "         657])\n",
            "  labels: 2\n",
            "tensor([[0.4026, 0.1697, 0.1654, 0.2624]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.3675, 0.1478, 0.3130, 0.1717]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.1546, 0.2629, 0.3221, 0.2604]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2128, 0.1024, 0.3707, 0.3141]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.3284, 0.1128, 0.1617, 0.3971]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2896, 0.3338, 0.1683, 0.2083]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2158, 0.1053, 0.3579, 0.3211]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.3438, 0.3193, 0.1999, 0.1371]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2216, 0.1593, 0.3849, 0.2342]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2946, 0.3198, 0.1568, 0.2288]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a9ea6c48951d>:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  'labels': torch.tensor(self.y[idx], dtype=torch.int64)\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no0XYdwhW7Cn"
      },
      "outputs": [],
      "source": [
        "#knock85\n",
        "\"\"\"\n",
        "85. 双方向RNN・多層化Permalink\n",
        "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n",
        "\n",
        "h←T+1=0,h←t=RNN←−−−(emb(xt),h←t+1),y=softmax(W(yh)[h→T;h←1]+b(y))\n",
        "ただし，h→t∈Rdh,h←t∈Rdh\n",
        "はそれぞれ，順方向および逆方向のRNNで求めた時刻t\n",
        "の隠れ状態ベクトル，RNN←−−−(x,h)\n",
        "は入力x\n",
        "と次時刻の隠れ状態h\n",
        "から前状態を計算するRNNユニット，W(yh)∈RL×2dh\n",
        "は隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈RL\n",
        "はバイアス項である．また，[a;b]\n",
        "はベクトルa\n",
        "とb\n",
        "の連結を表す。\n",
        "\n",
        "さらに，双方向RNNを多層化して実験せよ．\n",
        "\"\"\"\n",
        "class ID():\n",
        "    def __init__(self, data):\n",
        "        self.train_dict = defaultdict(int)\n",
        "        self.id_list = []\n",
        "        self.id_dict = dict()\n",
        "        self.make_id(data)\n",
        "\n",
        "    def make_id(self, data):\n",
        "        for line in data:\n",
        "            words = line.translate(table).split()\n",
        "            for word in words:\n",
        "                self.train_dict[word] += 1\n",
        "        calc_dict = dict(self.train_dict)\n",
        "        sort_list = sorted(calc_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "        for i, (trg_word, freq) in enumerate(sort_list):\n",
        "            if freq >= 2:\n",
        "                self.id_list.append((trg_word, i+1))\n",
        "            else:\n",
        "                self.id_list.append((trg_word, 0))\n",
        "        self.id_dict = dict(self.id_list)\n",
        "\n",
        "    def return_id(self, line):\n",
        "        one_hot_vec = []\n",
        "        words = line.strip().split(\" \")\n",
        "        for word in words:\n",
        "            if word in self.id_dict.keys():\n",
        "                one_hot_vec.append(self.id_dict[word])\n",
        "            else:\n",
        "                one_hot_vec.append(0)\n",
        "        return one_hot_vec\n",
        "\n",
        "\n",
        "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "\n",
        "header_name = ['TITLE', 'CATEGORY']\n",
        "train_file = \"train.txt\"\n",
        "train_data = pd.read_csv(train_file, header=None,\n",
        "                         sep='\\t', names=header_name)\n",
        "w2id = ID(train_data['TITLE'])\n",
        "test_vec = w2id.return_id(train_data[\"TITLE\"][10])\n",
        "print(train_data[\"TITLE\"][10])\n",
        "print(test_vec)\n",
        "\n",
        "# 追加\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(\n",
        "    './GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "VOCAB_SIZE = len(set(w2id.id_dict.keys())) + 1\n",
        "EMB_SIZE = 300\n",
        "weights = np.zeros((VOCAB_SIZE, EMB_SIZE))\n",
        "words_in_pretrained = 0\n",
        "for i, word in enumerate(w2id.id_dict.keys()):\n",
        "    try:\n",
        "        weights[i] = model[word]\n",
        "        words_in_pretrained += 1\n",
        "    except KeyError:\n",
        "        weights[i] = np.random.normal(scale=0.4, size=(EMB_SIZE,))\n",
        "weights = torch.from_numpy(weights.astype((np.float32)))\n",
        "\n",
        "print(f'learned word num: {words_in_pretrained} / {VOCAB_SIZE}')\n",
        "print(weights.size())\n",
        "\n",
        "\"\"\"81 RNNによる予測\"\"\"\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, emb_size, pad_idx, output_size, device, num_layers, emb_weight=None, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.hid_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_directions = bidirectional + 1\n",
        "        if emb_weight != None:\n",
        "            self.emb = nn.Embedding.from_pretrained(\n",
        "                emb_weight, padding_idx=pad_idx)\n",
        "        else:\n",
        "            self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
        "        # self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
        "        self.rnn = nn.RNN(emb_size, hidden_size, num_layers,\n",
        "                          nonlinearity=\"tanh\", batch_first=True, bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(hidden_size * self.num_directions, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.batch_size = x.size()[0]\n",
        "        hidden = torch.zeros(self.num_layers * self.num_directions,\n",
        "                             self.batch_size, self.hid_size, device=device)\n",
        "        emb = self.emb(x)\n",
        "        out, hidden = self.rnn(emb, hidden)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, x, y, tokenizer):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.x[idx]\n",
        "        inputs = self.tokenizer(text)\n",
        "\n",
        "        return {\n",
        "            'inputs': torch.tensor(inputs, dtype=torch.int64),\n",
        "            'labels': torch.tensor(self.y[idx], dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "\n",
        "train = pd.read_csv(\"train.txt\", sep='\\t')\n",
        "valid = pd.read_csv(\"valid.txt\", sep='\\t')\n",
        "test = pd.read_csv(\"test.txt\", sep='\\t')\n",
        "\n",
        "category = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
        "y_train = torch.tensor(train['CATEGORY'].map(lambda x: category[x]).values)\n",
        "y_valid = torch.tensor(valid['CATEGORY'].map(lambda x: category[x]).values)\n",
        "y_test = torch.tensor(test['CATEGORY'].map(lambda x: category[x]).values)\n",
        "\n",
        "dataset_train = NewsDataset(train[\"TITLE\"], y_train, w2id.return_id)\n",
        "dataset_valid = NewsDataset(valid[\"TITLE\"], y_valid, w2id.return_id)\n",
        "dataset_test = NewsDataset(test[\"TITLE\"], y_test, w2id.return_id)\n",
        "\n",
        "\"\"\"82 確率的勾配降下法による学習\"\"\"\n",
        "\n",
        "\n",
        "def calc_loss_acc(model, dataset, device=None, criterion=None):\n",
        "    # model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "    loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs = data[\"inputs\"].to(device)\n",
        "            labels = data[\"labels\"].to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            if criterion != None:\n",
        "                loss += criterion(outputs, labels).item()\n",
        "\n",
        "            pred = torch.argmax(outputs, dim=-1)\n",
        "            total += len(inputs)\n",
        "            correct += (pred == labels).sum().item()\n",
        "\n",
        "        return loss / len(dataset), correct / total\n",
        "\n",
        "\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion,  optimizer, num_epochs, device=None, collate_fn=None):\n",
        "    model.to(device)\n",
        "\n",
        "    dataloader_train = DataLoader(\n",
        "        dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    dataloader_valid = DataLoader(\n",
        "        dataset_valid, batch_size=1, shuffle=False)\n",
        "    # dataloader_valid = DataLoader(\n",
        "    #     dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "\n",
        "    log_train = []\n",
        "    log_valid = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        model.train()\n",
        "        loss_train = 0.0\n",
        "        for data in dataloader_train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs = data[\"inputs\"].to(device)\n",
        "            labels = data[\"labels\"].to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        end_time = time.time()\n",
        "\n",
        "        model.eval()\n",
        "        loss_train, acc_train = calc_loss_acc(\n",
        "            model, dataset_train, device, criterion=criterion)\n",
        "        loss_valid, acc_valid = calc_loss_acc(\n",
        "            model, dataset_valid, device, criterion=criterion)\n",
        "        log_train.append([loss_train, acc_train])\n",
        "        log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(\n",
        "        ), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "        print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, train_time: {(end_time - start_time):.4f}sec')\n",
        "    return {\n",
        "        \"train\": log_train,\n",
        "        \"valid\": log_valid\n",
        "    }\n",
        "\n",
        "\n",
        "def visualize_logs(log):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax[0].plot(np.array(log['train']).T[0], label='train')\n",
        "    ax[0].plot(np.array(log['valid']).T[0], label='valid')\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend()\n",
        "    ax[1].plot(np.array(log['train']).T[1], label='train')\n",
        "    ax[1].plot(np.array(log['valid']).T[1], label='valid')\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('accuracy')\n",
        "    ax[1].legend()\n",
        "    plt.savefig(\"83.png\")\n",
        "\n",
        "\n",
        "\"\"\"83 ミニバッチ化・GPU上での学習\"\"\"\n",
        "\n",
        "\n",
        "class Padseq():\n",
        "    def __init__(self, padding_idx):\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        sorted_batch = sorted(\n",
        "            batch, key=lambda x: x['inputs'].shape[0], reverse=True)\n",
        "        sequences = [x['inputs'] for x in sorted_batch]\n",
        "        sequences_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "            sequences, batch_first=True, padding_value=self.padding_idx)\n",
        "        labels = torch.LongTensor([x['labels'] for x in sorted_batch])\n",
        "\n",
        "        return {'inputs': sequences_padded, 'labels': labels}\n",
        "\n",
        "\n",
        "VOCAB_SIZE = len(set(w2id.id_dict.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(w2id.id_dict.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "NUM_LAYERS = 2\n",
        "LEARNING_RATE = 5e-2\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# def __init__(self, hidden_size, vocab_size, emb_size, pad_idx, output_size, device, num_layers, emb_weight=None, bidirectional=False):\n",
        "model = RNN(HIDDEN_SIZE, VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE,\n",
        "            device, NUM_LAYERS, emb_weight=weights, bidirectional=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion,\n",
        "                  optimizer, NUM_EPOCHS, device=device, collate_fn=Padseq(PADDING_IDX))\n",
        "\n",
        "visualize_logs(log)\n",
        "_, acc_train = calc_loss_acc(model, dataset_train, device)\n",
        "_, acc_test = calc_loss_acc(model, dataset_test, device)\n",
        "print(f'accuracy (train)：{acc_train:.3f}')\n",
        "print(f'accuracy (test)：{acc_test:.3f}')\n",
        "\n",
        "\"\"\"\n",
        "epoch: 1, loss_train: 1.3530, accuracy_train: 0.4072, loss_valid: 1.4054, accuracy_valid: 0.3982, train_time: 1.7147sec\n",
        "epoch: 2, loss_train: 1.2981, accuracy_train: 0.4228, loss_valid: 1.3499, accuracy_valid: 0.4087, train_time: 1.5063sec\n",
        "epoch: 3, loss_train: 1.1583, accuracy_train: 0.5153, loss_valid: 1.1957, accuracy_valid: 0.4835, train_time: 1.4932sec\n",
        "epoch: 4, loss_train: 1.1657, accuracy_train: 0.5111, loss_valid: 1.2189, accuracy_valid: 0.4790, train_time: 1.5019sec\n",
        "epoch: 5, loss_train: 1.1748, accuracy_train: 0.5269, loss_valid: 1.2493, accuracy_valid: 0.4903, train_time: 1.5028sec\n",
        "epoch: 6, loss_train: 1.2349, accuracy_train: 0.5168, loss_valid: 1.3283, accuracy_valid: 0.4775, train_time: 1.4967sec\n",
        "epoch: 7, loss_train: 1.1395, accuracy_train: 0.5477, loss_valid: 1.2095, accuracy_valid: 0.5135, train_time: 1.4977sec\n",
        "epoch: 8, loss_train: 1.1959, accuracy_train: 0.5025, loss_valid: 1.2595, accuracy_valid: 0.4686, train_time: 1.5069sec\n",
        "epoch: 9, loss_train: 1.3441, accuracy_train: 0.4355, loss_valid: 1.4145, accuracy_valid: 0.4214, train_time: 1.5221sec\n",
        "epoch: 10, loss_train: 1.2460, accuracy_train: 0.4896, loss_valid: 1.3213, accuracy_valid: 0.4693, train_time: 1.4973sec\n",
        "\n",
        "accuracy (train)：0.490\n",
        "accuracy (test)：0.455\n",
        "\"\"\""
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlWWAyi3aScB"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece"
      ],
      "metadata": {
        "id": "Y_wN86v4aeUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import optim\n",
        "from torch import cuda\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "train = pd.read_csv(\"./drive/MyDrive/data/train.txt\", sep='\\t')\n",
        "valid = pd.read_csv(\"./drive/MyDrive/data/valid.txt\", sep='\\t')\n",
        "test = pd.read_csv(\"./drive/MyDrive/data/test.txt\", sep='\\t')\n",
        "\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, x, y, tokenizer, max_len):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.x[idx]\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length = self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True\n",
        "            )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            \"ids\" : torch.LongTensor(ids),\n",
        "            \"mask\" : torch.LongTensor(mask),\n",
        "            \"labels\" : torch.Tensor(self.y[idx])\n",
        "        }\n",
        "\n",
        "\n",
        "y_train = pd.get_dummies(train, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
        "y_valid = pd.get_dummies(valid, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
        "y_test = pd.get_dummies(test, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
        "\n",
        "max_len = 20\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataset_train = NewsDataset(train['TITLE'], y_train, tokenizer, max_len)\n",
        "dataset_valid = NewsDataset(valid['TITLE'], y_valid, tokenizer, max_len)\n",
        "dataset_test = NewsDataset(test['TITLE'], y_test, tokenizer, max_len)\n",
        "\n",
        "for var in dataset_train[0]:\n",
        "  print(f'{var}: {dataset_train[0][var]}')\n",
        "\n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self, drop_rate, otuput_size):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.drop = torch.nn.Dropout(drop_rate)\n",
        "        self.fc = torch.nn.Linear(768, otuput_size)\n",
        "\n",
        "    def forward(self, ids, mask, return_dict):\n",
        "        _, out = self.bert(ids, attention_mask=mask,return_dict=return_dict)\n",
        "        out = self.fc(self.drop(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "def calculate_loss_and_accuracy(model, criterion, loader, device):\n",
        "    model.eval()\n",
        "    loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "          for data in loader:\n",
        "            ids = data['ids'].to(device)\n",
        "            mask = data['mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            outputs = model(ids, mask, return_dict=False)\n",
        "\n",
        "            loss += criterion(outputs, labels).item()\n",
        "\n",
        "            pred = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
        "            labels = torch.argmax(labels, dim=-1).cpu().numpy()\n",
        "            total += len(labels)\n",
        "            correct += (pred == labels).sum().item()\n",
        "\n",
        "    return loss / len(loader), correct / total\n",
        "\n",
        "\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
        "    model.to(device)\n",
        "\n",
        "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "    dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "\n",
        "    log_train = []\n",
        "    log_valid = []\n",
        "    for epoch in range(num_epochs):\n",
        "        s_time = time.time()\n",
        "\n",
        "        model.train()\n",
        "        for data in dataloader_train:\n",
        "            ids = data['ids'].to(device)\n",
        "            mask = data['mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(ids, mask, return_dict=False)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train, device)\n",
        "        loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid, device)\n",
        "        log_train.append([loss_train, acc_train])\n",
        "        log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "        e_time = time.time()\n",
        "\n",
        "        print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec')\n",
        "\n",
        "    return {'train': log_train, 'valid': log_valid}\n",
        "\n",
        "\n",
        "DROP_RATE = 0.4\n",
        "OUTPUT_SIZE = 4\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 4\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "model = BERTClass(DROP_RATE, OUTPUT_SIZE)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device=device)\n",
        "\n",
        "\n",
        "def visualize_logs(log):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax[0].plot(np.array(log['train']).T[0], label='train')\n",
        "    ax[0].plot(np.array(log['valid']).T[0], label='valid')\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend()\n",
        "    ax[1].plot(np.array(log['train']).T[1], label='train')\n",
        "    ax[1].plot(np.array(log['valid']).T[1], label='valid')\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('accuracy')\n",
        "    ax[1].legend()\n",
        "    plt.savefig(\"89.png\")\n",
        "\n",
        "\n",
        "visualize_logs(log)"
      ],
      "metadata": {
        "id": "DjBpX_Utae6X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}